{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcLWdUuikyzXW7qRpb/sCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning_2025/blob/main/PRACTICA_06_RNN_y_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"10\"><b>06. RNN y LSTM</b></font>\n",
        "\n",
        "Miguel A. Castellanos"
      ],
      "metadata": {
        "id": "JMIJIRvXAhWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJZrNIVSzHvX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a generar secuencias que provienen de un seno y vamos a intentar predecir el siguiente punto. No es una tarea complicada así que debería funcionar sin demasiada dimensionalidad.\n",
        "\n",
        "En este ejemplo vamos a ilustrar las aquitecturas many to one, many to many soncronizado y many to many no sincronizado\n"
      ],
      "metadata": {
        "id": "uQbD03TMRNQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>Many to One</b></font>"
      ],
      "metadata": {
        "id": "P9g6bhVfbTZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar datos senoidales con fase aleatoria\n",
        "# La fase es importante para que estén desplazadas en x\n",
        "\n",
        "# Si frequency es un numero genera todas las secuencias a esa frecuencia, si no\n",
        "# Las frecuencias son aleatorias entre 0.5 y 3.0\n",
        "def generate_sine_data(seq_length=10, num_samples=1000, frequency=None):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        phase_shift = np.random.uniform(0, 2 * np.pi)  # Fase aleatoria\n",
        "        current_frequency = frequency if frequency is not None else np.random.uniform(0.5, 3.0)  # Frecuencia fija o aleatoria\n",
        "\n",
        "        x_values = np.linspace(0, seq_length, seq_length) * (2 * np.pi * current_frequency / seq_length)  # Ajuste de la frecuencia\n",
        "        sine_wave = np.sin(x_values + phase_shift)\n",
        "\n",
        "        sequences.append(sine_wave)\n",
        "        targets.append(np.sin((seq_length * (2 * np.pi * current_frequency / seq_length)) + phase_shift))\n",
        "\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "\n",
        "# Crear dataset\n",
        "seq_length = 100     # numero de elementos en la secuencia\n",
        "num_samples = 1000  # numero de secuencias\n",
        "X, y = generate_sine_data(seq_length, num_samples)"
      ],
      "metadata": {
        "id": "hmHQ8R_eqLZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "qOXugDDvFYoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar una matriz de 4x4 plots eligiendo aleatoriamente las secuencias\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "random_indices = np.random.randint(0, num_samples, size=(3, 3))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        idx = random_indices[i, j]\n",
        "        axes[i, j].plot(range(100), X[idx,])\n",
        "        axes[i, j].scatter(100, y[idx,], color='orange')\n",
        "        axes[i, j].set_title(f\"Ejemplo {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rkh-EmlrSe6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos los tensores y el dataloader\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)  # (samples, seq_length, 1)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)  # (samples, 1)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "DQPkA6DkNfj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la red RNN\n",
        "# Ojo, fijate que no usamos sequential, ahora tenemos que definir mejor el forward\n",
        "\n",
        "# fijate que contiene el out, es una matriz de\n",
        "# out: Este es el tensor de salida de la RNN. Dependiendo de la implementación de la RNN, out tiene la forma [batch_size, seq_len, hidden_size]:\n",
        "\n",
        "\n",
        "# Definir la RNN\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=1, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Tomar la última salida de la secuencia\n",
        "        return out"
      ],
      "metadata": {
        "id": "Fadt2EuaNs3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El parámetro batch_first = true indica que el batch está contruido con la siguiente estructura [casos_del_batch, timestamps, variables_en_cada_timestamp]\n",
        "\n",
        "Fíjate en la definición de la red anterior:\n",
        "\n",
        "* input_size=1   # dimesiones de x\n",
        "* hidden_size=50 # neuronas en la RNN\n",
        "* num_layers=1   # numero de layers en stack\n",
        "\n",
        "\n",
        "Al ser una RNN con más de una neurona hemos introducido una capa de integración Z, es donde pone\n",
        "\n",
        "```\n",
        "self.fc = nn.Linear(hidden_size, 1)\n",
        "```\n",
        "\n",
        "Para cada uno de los pasos la red va a propagar toda la activación de sus neuronas al siguiente paso, pero además, con esas activaciones va a realizar una integración Z que va a ser el pronóstico yt de la red en ese paso temporal\n",
        "\n",
        "Como el objetivo es ***many to one*** lo que nos interesa es utilizar unicamente el último elemento de las predicciones de la RNN y entrenar la red con él.\n",
        "\n",
        "La RNN hace todos los pronósticos yt pero nosotros solo utilizamnos el último para el entrenamiento\n",
        "\n",
        "La sintaxis de python\n",
        "```\n",
        "out[:, -1, :]\n",
        "```\n",
        "\n",
        "es un modo avanzado de decirle que devuelva el último elemento de la segunda dimensión de la matriz out, justo con el que queremos entrenar\n",
        "\n",
        "Si quisiéramos quedarnos con los último 5 elementos sería\n",
        "\n",
        "```\n",
        "out[:, -5:, :]\n",
        "```\n",
        "\n",
        "Y si quisiéramos quedarnos con los elementos de 2 al 5\n",
        "\n",
        "```\n",
        "out[:, 2:5, :]\n",
        "```\n",
        "\n",
        "**Ojo, esto controla lo que devuelve el forward del modelo, luego tienes que hacer coindir con la dimensinalidad de y para que pueda calcularse el Loss**"
      ],
      "metadata": {
        "id": "F7r_wMrzTYzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo, función de pérdida y optimizador\n",
        "# Fijate que esto es igual que en MLP\n",
        "\n",
        "model = RNN()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "uhWnOG29OmPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar la RNN\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "PH2oAUeQOsjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "# vamos a generar 100 nuevos datos que provienen del mismo generador y ver cuanto acertamos\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X,_ = generate_sine_data(seq_length, 100)  # Generar datos de prueba\n",
        "    test_X_tensor = torch.tensor(test_X, dtype=torch.float32).unsqueeze(-1)\n",
        "    test_y = model(test_X_tensor).squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "8jRekehGQ641"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar una matriz de 3x3 plots eligiendo aleatoriamente las secuencias\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "random_indices = np.random.randint(0, 100, size=(3, 3))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        idx = random_indices[i, j]\n",
        "        axes[i, j].plot(range(100), test_X[idx,])\n",
        "        axes[i, j].scatter(100, test_y[idx,], color='orange')\n",
        "        axes[i, j].set_title(f\"Test {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QVgomnJaXHAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con una inspección visual es suficiente, si quieres añadir el cálculo del MSE o alguna otra métrica final puedes hacerlo"
      ],
      "metadata": {
        "id": "infFeI44ah0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>Many to Many sincronizado</b></font>\n",
        "\n",
        "Usando el mismo generador de senos vamos a predecir todos los puntos de la secuencia."
      ],
      "metadata": {
        "id": "sV_JEXXac0sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la RNN\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=1, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, :, :])  # Devolvemos toda la secuencia\n",
        "        return out"
      ],
      "metadata": {
        "id": "mV0RMABxVPKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo, función de pérdida y optimizador\n",
        "model = RNN()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "xQlKVwmvVUG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar la RNN\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    for batch_X,_ in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_X) # fijate que usamos como batch_y el propio batch_x\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "Xj1uceWLVZgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X, test_y = generate_sine_data(seq_length, 100)  # Generar datos de prueba\n",
        "    test_X_tensor = torch.tensor(test_X, dtype=torch.float32).unsqueeze(-1)\n",
        "    test_y = model(test_X_tensor).squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "CweJ9dczXHui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar una matriz de 3x3 plots eligiendo aleatoriamente las secuencias\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "random_indices = np.random.randint(0, 100, size=(3, 3))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        idx = random_indices[i, j]\n",
        "        axes[i, j].plot(range(100), test_X[idx,])\n",
        "        axes[i, j].scatter(range(100), test_y[idx,], color='orange')\n",
        "        axes[i, j].set_title(f\"Test {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wtmi-xuubzSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>Many to Many No Sincronizado</b></font>\n",
        "\n",
        "Usando el mismo generador de senos vamos a predecir solo una parte de los puntos de la secuencia."
      ],
      "metadata": {
        "id": "QU_qmvDxcnIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la RNN\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=1, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -5:, :])  # Tomar los últimos 5 puntos de la secuencia\n",
        "        return out"
      ],
      "metadata": {
        "id": "gQ2Ih-hSYIQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo, función de pérdida y optimizador\n",
        "model = RNN()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "i9YaLpRPYUQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar la RNN\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    for batch_X, _ in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch_X)\n",
        "        loss = loss_fn(predictions, batch_X[:,-5:,:])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "iYgLQrYaYUQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_X, test_y = generate_sine_data(seq_length, 100)  # Generar datos de prueba\n",
        "    test_X_tensor = torch.tensor(test_X, dtype=torch.float32).unsqueeze(-1)\n",
        "    test_y = model(test_X_tensor).squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "4UWdHHjtYUQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar una matriz de 3x3 plots eligiendo aleatoriamente las secuencias\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "random_indices = np.random.randint(0, 100, size=(3, 3))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        idx = random_indices[i, j]\n",
        "        axes[i, j].plot(range(100), test_X[idx,])\n",
        "        axes[i, j].scatter(range(95,100), test_y[idx,], color='orange')\n",
        "        axes[i, j].set_title(f\"Test {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_gNj8A9LiZ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>One to Many</b></font>\n",
        "\n",
        "Para calcular un One to Many la forma más sencilla de implementarlo es repetir la entrada tantos pasos temporales como necesites, es decir, si quieres una salida de 10 pasos creas un vector de 10 veces el numero x repetido\n",
        "\n",
        "Tienes dos estrategias para implementarlo:\n",
        "\n",
        "- Creas tus X y batch de esa manera (la más sencilla)\n",
        "- Haces que en el cálculo del forward se expanda el valor como un vector\n",
        "\n",
        "```\n",
        "def forward(self, x):\n",
        "  # Primero necesitamos expandir la secuencia de entrada a la longitud deseada\n",
        "  # La entrada x tiene forma [batch_size, 1, input_size] (una secuencia de 1 valor)\n",
        "  x = x.repeat(1, self.seq_len, 1)  # Repetimos la entrada a lo largo de la longitud de secuencia\n",
        "```\n"
      ],
      "metadata": {
        "id": "vWOzSCnAjKPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>LSTM y GRU</b></font>\n",
        "\n",
        "La implementación de las funciones LSTM y GRU es directa, simplemente sustituir la función RNN por estas funciones:\n",
        "\n",
        "```\n",
        "# Capa LSTM\n",
        "self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "```\n",
        "\n",
        "Con el parámetro hidden_size se controla la dimensionalidad oculta, esto determina el tamaño de la celda oculta y por tanto también el número de neuronas necesarias para las puertas forget, input y output.\n",
        "\n",
        "```\n",
        "# Capa GRU\n",
        "self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "```\n",
        "\n",
        "En la capa GRU la dimensionalidad oculta determina el número de neuronas pero no de la celda de memoria porque no existe.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K8fRygFhkgOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>Múltiples variables, layers apilados y bidireccional</b></font>\n",
        "\n",
        "Esta explicación vale para las funciones de RNN, LSTM y GRU\n",
        "\n",
        "Introducir múltiples variables es directo, simplemente tienes que construir tu matriz de datos haciendo que la tercera dimensión contenga las covariables. Es decir si vas a tener 100 casos, en 10 pasos de tiempo y 2 variables debes acabar con una matriz de dimensiones:\n",
        "\n",
        "\n",
        "[100, 10, 2] es decir [casos_del_batch, timestamps, variables_en_cada_timestamp]\n",
        "\n",
        "\n",
        "Para introducir layer apilados simplemente cambia el parámetro num_layers de 1 al valor que quieras, recuerda que eso puede incrementar significativamente el tiempo de ejecución\n",
        "\n",
        "\n",
        "Para hacer que la red tenga un aprendizaje bidireccional simplemente se introduce en la función el parámetro bidirectional=True, por ejemplo:\n",
        "\n",
        "```\n",
        "self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "```\n"
      ],
      "metadata": {
        "id": "o60U4LprnFSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"5\"><b>Ejercicio 01</b></font>\n",
        "\n",
        "El objetivo es ser capaces de \"hackear\" el generador aleatorio de python.\n",
        "\n",
        "Fíjate en el siguiente código. La función va a generar semillas aleatorias y en función de esas semillas va a producir una secuencia aleatoria.\n",
        "\n",
        "El objetivo es, conociendo la semilla ser capaz de producir la misma secuencia, si haces esto, aunque la \"fórmula generadora\" de python sea secreta tu la habrás \"descubierto\"\n",
        "\n",
        "Tarea: Construye un modelo secuencial de la forma que sea para descubir la ecuación generadora de python\n",
        "\n"
      ],
      "metadata": {
        "id": "x-5MHZGJpT2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_random_vectors(num_vectors = 1000, num_seq = 10):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    # Generar vectores de 10 números aleatorios con semillas diferentes\n",
        "    seeds = []\n",
        "    vectors = []\n",
        "\n",
        "    for i in range(num_vectors):\n",
        "        # Generar una semilla aleatoria para cada vector\n",
        "        seed = np.random.randint(0, 2**32)\n",
        "        seeds.append(seed)\n",
        "        np.random.seed(seed)  # Establecer la semilla para la generación de números aleatorios\n",
        "        vector = np.random.rand(num_seq)  # Generar el vector de 10 números aleatorios\n",
        "        vectors.append(vector)  # Almacenar la semilla y el vector\n",
        "    return seeds, vectors\n",
        "\n",
        "\n",
        "X, y = generate_random_vectors()\n",
        "\n"
      ],
      "metadata": {
        "id": "ljLjoUltr1OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"5\"><b>Ejercicio 02</b></font>\n",
        "\n",
        "Usando redes RNN y LSTM pon un ejemplo en el que se compruebe como en las RNN los estados inciales dejan de influir en la predicción final si la secuencia es larga y como eso se resuelve usando una LSTM"
      ],
      "metadata": {
        "id": "uDLDbW-5wNoj"
      }
    }
  ]
}