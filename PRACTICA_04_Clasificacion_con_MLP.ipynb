{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5O2sD7RccLQF/AjHytNpm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning_2025/blob/main/PRACTICA_04_Clasificacion_con_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"10\"><b>04. Clasificación con MLP</b></font>\n",
        "\n",
        "Miguel A. Castellanos"
      ],
      "metadata": {
        "id": "BQI71TuMRsmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta práctica vamos a hacer clasificaciones binarias y múltiples\n",
        "\n",
        "Son similares a las hechas con playground pero ahora crearemos y controlaremos nosotros los datos."
      ],
      "metadata": {
        "id": "-Rki5pOqRyA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi5DBZdtRkiY"
      },
      "outputs": [],
      "source": [
        "# Vamos a empezar con 2 pelotas\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "X, y = make_blobs(centers=2, cluster_std=0.8)\n",
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"0\")\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"1\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a instalar torcheval, es parte de torch y va a facilitar la evaluacion del modelo\n",
        "\n",
        "!pip install torcheval"
      ],
      "metadata": {
        "id": "W0FPfIImSVZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparamos los datos como antes, pero y no hay que normalizarla\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Normalizamos solo X\n",
        "\n",
        "x_mean, x_std = X.mean(), X.std()\n",
        "X_norm = (X - x_mean) / x_std\n",
        "\n",
        "tensor_X = torch.Tensor(X_norm)\n",
        "tensor_y = torch.Tensor(y)\n",
        "\n",
        "my_dataset = TensorDataset(tensor_X,tensor_y) # create your datset\n",
        "my_dataloader = DataLoader(my_dataset) # create your dataloader"
      ],
      "metadata": {
        "id": "FC06JSWoSfp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos mi red igual que antes\n",
        "# Es una red con 3 capas (50,25,1), todo relu menos la ultima capa que hace\n",
        "# la clasificación binaria con una sigmoide\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.all_layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_features, 50),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(50, 25),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(25, 1),\n",
        "            # torch.nn.Sigmoid() # ver comentario 1\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.all_layers(x)\n",
        "        # return output\n",
        "        return output.flatten() # ver comentario 2"
      ],
      "metadata": {
        "id": "vTEndzQES2vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comentario 1\n",
        "\n",
        "tenemos dos opciones, o definimos en la última capa la función sigmoid y usamos como función de costo BCELoss\n",
        "\n",
        "```python\n",
        "torch.nn.Sigmoid()\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "```\n",
        "\n",
        "O no definimmos sigmoid y usamos BCEWithLogitsLoss\n",
        "\n",
        "```python\n",
        "# torch.nn.Sigmoid()\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "```\n",
        "\n",
        "\n",
        "Esta segunda opción funciona mucho mejor pero hay que tener en cuenta que va a devolver valores que no están entre [0,1] sino [-ing,+inf]\n",
        "\n",
        "\n",
        "## Comentario 2\n",
        "Ojo con las dimnsiones de entrada, salida y las internas\n",
        "Por ejemplo en este codigo la red devuelve una lista (10,1) (una matriz) y los rasgos son (10,) (un vector)\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "  output = self.all_layers(x)\n",
        "  return output\n",
        "```\n",
        "\n",
        "Siempre hay funciones que necesitan una cosa pero tú les pasas otra y hay que ajustarlas, en este caso hemos optado por \"aplanar\" la salida de la red para que coincida con el cálculo del accurary\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "  output = self.all_layers(x)\n",
        "  return output.flatten()\n",
        "```\n"
      ],
      "metadata": {
        "id": "CvdKEJudg_40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torcheval.metrics import BinaryAccuracy, BinaryConfusionMatrix\n",
        "\n",
        "# definimos hiper-parámetros\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "model = MLP(num_features=2) # instanciamos el modelo\n",
        "\n",
        "# loss_fn = torch.nn.BCELoss() # Ojo, ahora es BCE, necesita sigmoid\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss() # Ojo, ahora es BCELog, no necesita sigmoid\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # usamos Adam\n",
        "\n",
        "acc = BinaryAccuracy() # calculamos la precisión (accurary)\n",
        "m_confusion = BinaryConfusionMatrix(threshold=0.5)\n"
      ],
      "metadata": {
        "id": "kPvjY2LZTcWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# vamos a guardar el lost pero tambien el accuracy\n",
        "loss_list, acc_list = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(my_dataloader):\n",
        "\n",
        "        # forward\n",
        "        output = model(features)\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Esto es algo especifico para la evaluacion, en cada epoca actualizamos:\n",
        "        acc.update(output, targets)      # el accuracy\n",
        "\n",
        "        # añadimos los nuevos valores a la lista\n",
        "        loss_list.append(loss.item())\n",
        "        acc_list.append(acc.compute().item())\n",
        "\n",
        "        if not batch_idx % 100:\n",
        "            ### Imprimimos info\n",
        "            print(\n",
        "                f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "                f\" | Batch {batch_idx:03d}/{len(my_dataloader):03d}\"\n",
        "                f\" | Train Loss: {loss:.2f}\"\n",
        "                f\" | Accuracy: {acc.compute():.2f}\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "v4tciS5UTxDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos que predicciones hace nuestro modelo para nuestro X\n",
        "\n",
        "model.eval()\n",
        "yp = model(tensor_X)\n",
        "yp"
      ],
      "metadata": {
        "id": "yZVROFOjZqVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos una matriz de confusion\n",
        "\n",
        "k = BinaryConfusionMatrix(threshold=0.)\n",
        "k.update(yp, tensor_y.long())\n",
        "print(k.compute())"
      ],
      "metadata": {
        "id": "77ZXcNafZqYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploteamos\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.plot(acc_list)\n",
        "plt.legend(['loss', 'accuracy'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MsUkt07WZqlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Ejercicios\n",
        "\n",
        "Crea un notebook de colab con las soluciones a los ejercicios, hazlo público y sube el enlace al campus virtual\n"
      ],
      "metadata": {
        "id": "yJTQc1u4tSIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01. Ejercicio 1\n",
        "\n",
        "Usando el generador de pelotas make_blobs crea unos datos con 4 categorías y calcula un MLP para hacer la clasificación.\n",
        "\n",
        "Ahora tendrás que modificar como ploteas\n",
        "\n",
        "Y también la última función de activación de la red será una SoftMax\n",
        "\n",
        "Al igual que con BCE, se prefiere la opción de hacer implicita esas funciones de activación y que sea la función que calcula la función de coste la que las incluya, es decir:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "# no se pone la softmax porque va incluida en el optimizador\n",
        "# torch.nn.Softmax(dim=4)\n",
        "\n",
        "...\n",
        "\n",
        "# ya incluye automaticamente las softmax al definir fn_loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss()  \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yLA3gKsItVb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(centers=4, cluster_std=0.8)\n",
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"0\")\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"1\")\n",
        "plt.scatter(X[y == 2, 0], X[y == 2, 1], label=\"2\")\n",
        "plt.scatter(X[y == 3, 0], X[y == 3, 1], label=\"3\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RAW0TiAidQ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pasa lo mismo, no se pone la capa de softmax, va implicita al definir la funcion criterion = nn.CrossEntropyLoss()  # Combina softmax y NLLLoss\n",
        "\n",
        "\n",
        "\n",
        "ahora la matriz de confusion es MulticlassConfusionMatrix(4)"
      ],
      "metadata": {
        "id": "GjEaFq6lo4gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02. Ejercicio 2\n",
        "\n",
        "Haz lo mismo usando make_circles\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_circles\n",
        "```\n"
      ],
      "metadata": {
        "id": "1kxxTBn-p3Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 03. Ejercicio 3\n",
        "\n",
        "Haz lo mismo usando make_moons\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "```"
      ],
      "metadata": {
        "id": "meFENizAmfMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04. Ejercicio 04. Clasificación binaria con datos de infidelidad\n",
        "\n",
        "Se analizan unos datos obtenido de Kaggle.com sobre una encuesta de infidelidad: [enlace](https://www.google.com/url?q=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Futkarshx27%2Ffairs-extramarital-affairs-data)\n",
        "\n",
        "Las variables son:\n",
        "\n",
        "**Criterio**: affairs.\n",
        "\n",
        "**Predictoras**: gender, age, yearsmarried, children, religiousness, education, occupation, rating.\n",
        "\n",
        "La variable criterio es de tipo dicotómico **0 = no, 1 = sí** por lo que el análisis convencional nos lleva a un modelo binomial. La precisión (accuracy) en la clasificación obtenida con este modelo es de 0.72\n",
        "\n",
        "Construye un MLP que supere la precisión del modelo lineal"
      ],
      "metadata": {
        "id": "8bTQancLnYic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mcstllns/UNIR2024/main/data-affairs.csv'\n",
        "data  = pd.read_csv(url)\n",
        "print(data.keys())\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "qcZiAQE9nvpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('affairs', axis=1)\n",
        "y = data['affairs']\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "rGBBj3t6n_i9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}