{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjxGgH7NlflwVp+ThBJ3fW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning_2025/blob/main/PRACTICA_05_Ajuste_de_la_red.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"10\"><b>05. Ajuste de la red</b></font>\n",
        "\n",
        "Miguel A. Castellanos"
      ],
      "metadata": {
        "id": "bwQAgJi6aSCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estocástico, Batch y mini-batch\n",
        "\n",
        "Esto define si queremos un aprendizaje estocástico (actualización cada sujeto), por batch (actualización con el promedio de todos los sujetos) o por mini-batch (actualización por lotes de sujetos).\n",
        "\n",
        "Se determina simplemente usando la función DataLoader"
      ],
      "metadata": {
        "id": "BslBU8fOTt7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset) # estocastico\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # mini-batch\n",
        "dataloader = DataLoader(dataset, batch_size=len(my_dataset), shuffle=True) # batch completo\n"
      ],
      "metadata": {
        "id": "IFumut4yqzqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizadores\n",
        "\n",
        "## SGD\n",
        "\n",
        "\n",
        "```python\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
        "```\n",
        "\n",
        "Siendo:\n",
        "- lr: Learning rate\n",
        "- momentum: Beta de momentum\n",
        "- weight_decay: el lambda2 de la Regularización L2\n",
        "- nesterov: Si está en True se activa la aceleración de nesterov\n",
        "\n",
        "### Aceleración de Nesterov\n",
        "Cuando activas nesterov=True, el optimizador ajusta el cálculo del momentum para \"mirar hacia adelante\" antes de actualizar los pesos. Esto puede hacer que el entrenamiento sea más estable y rápido. Requiere que el momentum sea mayor que 0 (momentum > 0), de lo contrario, no tiene efecto.\n",
        "\n",
        "SGD hará aprendizaje estocástico si no definimos mini-batch. Si hacemos n=n1 o n=N hará mini-batchs o batch\n",
        "\n",
        "Básico y funciona bien. Muy sensible al Learning Rate.\n",
        "\n",
        "\n",
        "## RMSprop\n",
        "\n",
        "```python\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0.9)\n",
        "```\n",
        "- lr:\tTasa de aprendizaje. Controla el tamaño de los pasos en la actualización de pesos.\n",
        "- alpha:\tFactor de suavizado (0.99 por defecto). Controla cuánto contribuyen los gradientes pasados a la media móvil. Es lo que hemos llamado Beta en teoría\n",
        "- eps:\tTérmino de estabilidad. Evita divisiones por cero (típicamente un valor muy muy pequeño, 1e-8). Es l oque hemos llamado epsilon en teoría.\n",
        "- weight_decay:\tRegularización L2. Similar a la regularización en SGD, evita sobreajuste.\n",
        "- momentum:\tAñade momento (por defecto 0). Se puede usar para mejorar convergencia.\n",
        "- centered:\tSi es True, usa una media móvil del gradiente para normalizar mejor. No se usa mucho.\n",
        "\n",
        "Funciona bien en problemas longitudinales como redes recurrentes (RNNs).\n",
        "No siempre converge a la mejor solución (a veces encuentra mínimos locales).\n",
        "Menos robusto en redes muy profundas comparado con Adam.\n",
        "\n",
        "\n",
        "## Adam\n",
        "\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)\n",
        "```\n",
        "\n",
        "- lr:\tTasa de aprendizaje (0.001 por defecto).\n",
        "- betas:\tFactores de decaimiento para momentum beta1 (0.9) y el segundo beta2 controla controla el beta del RMSProp\n",
        "- eps:\tTérmino de estabilidad (1e-8). Evita divisiones por cero.\n",
        "- weight_decay:\tRegularización L2 (por defecto 0)\n",
        "\n",
        "Es el que mejor funciona porque combina todo lo anterior. Requiere mas memoria que los anteriores y más cálculo. Hay que elegir sabiamente las betas para que vaya bien."
      ],
      "metadata": {
        "id": "Jh2u15C6KEOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicializacion de parametros\n",
        "\n",
        "Por defecto utiliza la siguiente inicialización y funciona muy bien, la verdad.\n",
        "\n",
        "\n",
        "| Capa | Inicialización por defecto |\n",
        "|------|----------------------------|\n",
        "|nn.Linear |\tUniforme en [-√k, √k], donde k = 1/in_features|\n",
        "| nn.Conv2d\t| Uniforme en [-√k, √k], donde k = 1/in_features|\n",
        "| nn.BatchNorm\t| pesos = 1, sesgos = 0 |\n",
        "\n",
        "Si se quiere utilizar las inicializaciones de Xavier, He y LeCun puede hacerse lo siguiente:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n0cYZIsgOgnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, X_nvars):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # Hidden Layer 1\n",
        "            torch.nn.Linear(X_nvars, 1), # Hacemos el sumatorio\n",
        "            torch.nn.ReLU(),              # Aplicamos la función de activacion\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(1, 1),\n",
        "        )\n",
        "\n",
        "        # Aqui es donde hacemos las incializaciones\n",
        "        for layer in self.layers:\n",
        "          if isinstance(layer, nn.Linear):\n",
        "            init.xavier_uniform_(layer.weight)# Si queremos xavier con uniforme\n",
        "            # init.xavier_normal_(layer.weight) # Si queremos xavier con normal\n",
        "\n",
        "            # init.kaiming_uniform_(layer.weight, nonlinearity='relu') # Si queremos He con uniforme\n",
        "            # init.kaiming_normal_(layer.weight, nonlinearity='relu') # Si queremos He con normal\n",
        "\n",
        "            # # LeCun normal\n",
        "            # init.normal_(layer.weight, mean=0, std=(1.0 / torch.sqrt(torch.tensor(layer.weight.size(1), dtype=torch.float))))\n",
        "\n",
        "            # # Inicialización LeCun uniforme (no hay una función directa, pero se puede hacer manualmente)\n",
        "            # fan_in = layer.weight.size(1)  # Número de entradas\n",
        "            # bound = 1 / torch.sqrt(torch.tensor(fan_in, dtype=torch.float))\n",
        "            # init.uniform_(layer.weight, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layers(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "nmN_uIUSOkbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear conjuntos de train, dev y test\n",
        "\n",
        "Lo primero, hay que tener en cuenta estas dos funciones:\n",
        "\n",
        "```python\n",
        "model.train()\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "Ponen al modelo en modo entrenamiento o de evaluación y tiene efecto sobre algunas cosas del funcionamiento de la red, por ejemplo si se hacen o no los dropout, si se calcula el gradiente, etc.\n",
        "\n",
        "La división entre los conjuntos de datos se puede hacer cómo quieras, hay múltiples funciones para ello. Pytorch incorpora el random_split\n"
      ],
      "metadata": {
        "id": "KGRgO2mqRbzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GzIGiBQCRb7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "X = torch.randn(1000, 10)\n",
        "y = torch.randint(0, 2, (1000,))  # Clasificación binaria\n",
        "\n",
        "dataset = TensorDataset(X, y) # create your datset\n",
        "\n",
        "# Definir tamaños de cada conjunto\n",
        "train_size = int(0.8 * len(dataset))  # 80% para entrenamiento\n",
        "dev_size = int(0.1 * len(dataset))  # 10% para validación\n",
        "test_size = len(dataset) - train_size - dev_size  # 10% para prueba\n",
        "\n",
        "# Dividir aleatoriamente\n",
        "train_dataset, dev_dataset, test_dataset = random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "# Crear dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dev_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "bmm5TWgiSkTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "\n",
        "Simplemente se introduce una capa de dropout después de la activación. El parámetro p define la cantidad de datos que queremos perder.\n",
        "\n",
        "- Si lo aplicamos antes de la activación, algunas neuronas pueden ser eliminadas antes de calcular su valor útil, perdiendo información antes de que sea procesada correctamente.\n",
        "- Aplicarlo después de la activación asegura que cada neurona ya tiene su contribución calculada antes de ser descartada temporalmente.\n"
      ],
      "metadata": {
        "id": "wIEtWqdiWBL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropout_p es la probabilidad de apagar una neurona (50%)\n",
        "dropout_p = 0.5\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, dropout_p):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Capa de entrada a capa oculta 1\n",
        "            nn.Linear(input_size, hidden_size_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),  # Dropout después de la capa oculta 1\n",
        "\n",
        "            # Capa oculta 1 a capa oculta 2\n",
        "            nn.Linear(hidden_size_1, hidden_size_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),  # Dropout después de la capa oculta 2\n",
        "\n",
        "            # Capa oculta 2 a capa de salida\n",
        "            nn.Linear(hidden_size_2, output_size),\n",
        "            nn.Sigmoid()  # Para clasificación binaria, puedes usar softmax para clasificación múltiple\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "kzKQ4sxxWc--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch-normalization\n",
        "\n",
        "Batch Normalization es una técnica para normalizar (media = 0 y sd = 1) las activaciones de cada capa en mini-batches. Esto ayuda a acelerar el entrenamiento y a estabilizar la red. En PyTorch, puedes usar torch.nn.BatchNorm1d, BatchNorm2d, o BatchNorm3d, dependiendo de la dimensión de los datos que estés manejando.\n",
        "\n",
        "\n",
        "Ademas, la función BatchNorm de pytorch introduce dos nuevos parámetros:\n",
        "\n",
        "- β (bias): permite un desplazamiento en la media.\n",
        "- γ (scale factor): escala la distribución normalizada.\n",
        "\n",
        "Es decir, se le puede decir que en vez de acabar en (0,1) se reescale a y (β,γ). Esto permite que la red aprenda si necesita reescalar o desplazar los valores."
      ],
      "metadata": {
        "id": "eJl2f7lGXOmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Capa de entrada a capa oculta 1\n",
        "            nn.Linear(input_size, hidden_size_1),\n",
        "            nn.BatchNorm1d(hidden_size_1),  # Normalización después de la capa 1\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Capa oculta 1 a capa oculta 2\n",
        "            nn.Linear(hidden_size_1, hidden_size_2),\n",
        "            nn.BatchNorm1d(hidden_size_2),  # Normalización después de la capa 2\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Capa oculta 2 a capa de salida\n",
        "            nn.Linear(hidden_size_2, output_size),\n",
        "            nn.Sigmoid()  # Para clasificación binaria\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Y0BPmlO4XeFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularizaciones L1 y L2\n",
        "\n",
        "Lo más eficiente es usar L2 con el weight dacay de los optimizadores, pero si quieres programarla a pelo, o programar una elasticNet, puedes hacerlo:"
      ],
      "metadata": {
        "id": "8W_0j8ikXxLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define ElasticNet\n",
        "# Si haces l1_lambda o l2_lambda = 0 ese parámetro no influye\n",
        "def l1_l2_regularization(model, l1_lambda, l2_lambda):\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters())  # L1\n",
        "    l2_norm = sum(p.pow(2).sum() for p in model.parameters())  # L2\n",
        "    return l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(my_dataloader):\n",
        "\n",
        "        # forward\n",
        "        output = model(features)\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        # La regularización\n",
        "        reg_loss = l1_l2_regularization(model, l1_lambda, l2_lambda)\n",
        "        total_loss = loss + reg_loss\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "d6pESIoF38CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usar la GPU\n",
        "\n",
        "Lo primero es crear un entorno de ejecución con una GPU\n",
        "\n",
        "Conectar -> Cambiar tipo de entorno de ejecución\n",
        "\n",
        "Y elegir GPU-T4"
      ],
      "metadata": {
        "id": "fPyEjat35LL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobamos si tenemos GPU\n",
        "import torch\n",
        "print(torch.cuda.is_available())  # Salida: True si hay una GPU disponible"
      ],
      "metadata": {
        "id": "XHOrUYwx5yHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si hay cuda indicamos que la GPU va a ser nuestro dispositivo de calculo, si no, la CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "NuqI1ZHv57gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si tentemos cuda ponemos imprimir informacion\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
      ],
      "metadata": {
        "id": "08HV6EfZ6TsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta función nos dice cosas de la Tarjeta grafica\n",
        "\n",
        "if device.type == 'cuda':\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "8KOEXY5l6Ym5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como lanzar un MLP sobre la GPU\n",
        "\n",
        "# Lo primero es pasar todos los tensores a la GPU\n",
        "\n",
        "# Dependiendo de la cantidad de memoria se recomiendan dos opciones:\n",
        "# A. Pasamos todos los datos a la GPU desde el principio (si tenemos mucha Vram)\n",
        "\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# DataLoader sin necesidad de mover lotes a GPU\n",
        "dataloader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n",
        "\n",
        "# B. Vamos pasando los mini-batch según los vayamos usando\n",
        "for epoch in range(5):\n",
        "    for x_batch, y_batch in dataloader:\n",
        "\n",
        "        # Mover solo el batch actual a GPU\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "\n",
        "# Lo segundo ejecutar el modelo sobre la GPU\n",
        "model = MLP().to(device)\n",
        "\n",
        "# Y el resto del código sería igual que en otras ocasiones"
      ],
      "metadata": {
        "id": "RtxRTl6h6zPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Búsqueda de hiper-parámetros con Optuna\n",
        "\n",
        "Vamos a partir de un MLP que ya vimos en la PRACTICA 04. Una red para clasificación binaria\n"
      ],
      "metadata": {
        "id": "fnvPVbgx-Ysj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "MSTruuRO-fGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos unos datos\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "import torcheval\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torcheval.metrics import BinaryAccuracy\n",
        "\n",
        "X = torch.randn(1000, 10)\n",
        "y = torch.randint(0, 2, (1000,))  # Clasificación binaria\n",
        "\n",
        "train_dataset = TensorDataset(X, y)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "x5nKVvMW-p0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el MLP\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.all_layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_features, 25),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(25, 1),\n",
        "            # torch.nn.Sigmoid() # ver comentario 1\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.all_layers(x)\n",
        "        return output.flatten()\n",
        "\n"
      ],
      "metadata": {
        "id": "qTxjtlpC_thD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define Optuna\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Aqui es donde se especifican los parametros que vamos a optimizar\n",
        "    # Vamos a optimizar solo el lr del optimizador\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.001, 0.01)\n",
        "\n",
        "    # Crear modelo con los hiperparámetros sugeridos\n",
        "    model = MLP(num_features = 10)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    accuracy = BinaryAccuracy()\n",
        "\n",
        "    model.train()\n",
        "    # Entrenamiento simple\n",
        "    for epoch in range(5):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = loss_fn(output, y_batch.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluar precisión\n",
        "    model.eval()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        output = model(X_batch)\n",
        "        accuracy.update(output, y_batch)\n",
        "\n",
        "    return accuracy.compute()  # Optuna intentará maximizar esto\n"
      ],
      "metadata": {
        "id": "vzMZMOSv_2Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")  # Buscamos maximizar la precisión\n",
        "study.optimize(objective, n_trials=20)  # Probar 20 combinaciones\n",
        "\n",
        "# 🔹 Mostrar los mejores hiperparámetros encontrados\n",
        "print(\"Mejores hiperparámetros:\", study.best_params)"
      ],
      "metadata": {
        "id": "dm1YYOO2CVpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.matplotlib.plot_param_importances(study)"
      ],
      "metadata": {
        "id": "TSuHNbu8ImkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.matplotlib.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "rw1d4jZRIsPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio\n",
        "\n",
        "Para esta práctica vamos a utilizar el conjunto de datos MNIST que consiste en imágenes de números del 0 al 9 escritas a mano y digitalizadas. Una descripción del fichero la puedes encontrar en la [wikipedia](https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "Los datos están ya almacenados en pytorch, con lo que con una única función los podemos cargar y, además, están ya divididos en dos conjuntos de datos, un de entrenamiento con 60000 imágenes y otro de test con 10000 imágenes.\n",
        "\n",
        "El objetivo de la red es ser capaz de identificar correctamente el número y eso es equivalente a clasificar correctamente cada imagen.\n",
        "\n",
        "Los datos de entrada son imágenes en escala de grises, una matriz bidimensional de 28 x 28 en la que cada pixel va de 0 a 255. La mejor manera de trabajar con imágenes es a través de redes convolucionales pero todavía no las hemos estudiado así que vamos a vectorizar la imagen, es decir, concatenar las columnas una tras otra y formas un unico vector para cada imagen.\n",
        "\n",
        "Para procesar los datos vamos a construir un perceptron multicapa como los de los ejercicios anteriores pero ahora como entrada vamos a tener\n",
        "\n",
        "- **Para el train:** 60000 imágenes y sus etiquetas\n",
        "- **Para el test:** 10000 imágenes y sus etiquetas\n"
      ],
      "metadata": {
        "id": "cN5HlhpaJELi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aEhsSamZKn7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(object):\n",
        "    def __call__(self, tensor):\n",
        "        # Aplanar la imagen a un vector de 784 píxeles\n",
        "        return tensor.view(-1)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convierte la imagen a tensor\n",
        "    transforms.Normalize((0.5,), (0.5,)),  # Normalización\n",
        "    transforms.Lambda(lambda x: x.view(-1)) # Aplanar las imágenes a un vector de 784 píxeles\n",
        "    # Flatten()  # Aplanar las imágenes a un vector de 784 píxeles\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "tC9auyXdL6pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener una imagen y su etiqueta\n",
        "image, label = train_dataset[5]  # Obtener la primera imagen del conjunto de entrenamiento\n",
        "\n",
        "# Visualizar la imagen\n",
        "plt.imshow(image.view(28,28), cmap='gray')  # .squeeze() elimina la dimensión de los canales (1,28,28 -> 28,28)\n",
        "plt.title(f\"Etiqueta: {label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qosa8eDoO5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 01\n",
        "\n",
        "Crea un MLP que realice una clasificación razonable, para ello utiliza las estrategias ***que consideres oportunas***.\n",
        "\n",
        "- Crear conjuntos de entrenamiento, dev y test\n",
        "- Ejecuta la red sobre una GPU\n",
        "- Buscar parámetros con Optuna\n",
        "- Usar regularizaciones\n",
        "- etc."
      ],
      "metadata": {
        "id": "ZyJ76ryOJflC"
      }
    }
  ]
}