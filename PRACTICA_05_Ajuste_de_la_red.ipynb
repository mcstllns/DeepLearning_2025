{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjxGgH7NlflwVp+ThBJ3fW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning_2025/blob/main/PRACTICA_05_Ajuste_de_la_red.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"10\"><b>05. Ajuste de la red</b></font>\n",
        "\n",
        "Miguel A. Castellanos"
      ],
      "metadata": {
        "id": "bwQAgJi6aSCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estoc谩stico, Batch y mini-batch\n",
        "\n",
        "Esto define si queremos un aprendizaje estoc谩stico (actualizaci贸n cada sujeto), por batch (actualizaci贸n con el promedio de todos los sujetos) o por mini-batch (actualizaci贸n por lotes de sujetos).\n",
        "\n",
        "Se determina simplemente usando la funci贸n DataLoader"
      ],
      "metadata": {
        "id": "BslBU8fOTt7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset) # estocastico\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # mini-batch\n",
        "dataloader = DataLoader(dataset, batch_size=len(my_dataset), shuffle=True) # batch completo\n"
      ],
      "metadata": {
        "id": "IFumut4yqzqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizadores\n",
        "\n",
        "## SGD\n",
        "\n",
        "\n",
        "```python\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
        "```\n",
        "\n",
        "Siendo:\n",
        "- lr: Learning rate\n",
        "- momentum: Beta de momentum\n",
        "- weight_decay: el lambda2 de la Regularizaci贸n L2\n",
        "- nesterov: Si est谩 en True se activa la aceleraci贸n de nesterov\n",
        "\n",
        "### Aceleraci贸n de Nesterov\n",
        "Cuando activas nesterov=True, el optimizador ajusta el c谩lculo del momentum para \"mirar hacia adelante\" antes de actualizar los pesos. Esto puede hacer que el entrenamiento sea m谩s estable y r谩pido. Requiere que el momentum sea mayor que 0 (momentum > 0), de lo contrario, no tiene efecto.\n",
        "\n",
        "SGD har谩 aprendizaje estoc谩stico si no definimos mini-batch. Si hacemos n=n1 o n=N har谩 mini-batchs o batch\n",
        "\n",
        "B谩sico y funciona bien. Muy sensible al Learning Rate.\n",
        "\n",
        "\n",
        "## RMSprop\n",
        "\n",
        "```python\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0.9)\n",
        "```\n",
        "- lr:\tTasa de aprendizaje. Controla el tama帽o de los pasos en la actualizaci贸n de pesos.\n",
        "- alpha:\tFactor de suavizado (0.99 por defecto). Controla cu谩nto contribuyen los gradientes pasados a la media m贸vil. Es lo que hemos llamado Beta en teor铆a\n",
        "- eps:\tT茅rmino de estabilidad. Evita divisiones por cero (t铆picamente un valor muy muy peque帽o, 1e-8). Es l oque hemos llamado epsilon en teor铆a.\n",
        "- weight_decay:\tRegularizaci贸n L2. Similar a la regularizaci贸n en SGD, evita sobreajuste.\n",
        "- momentum:\tA帽ade momento (por defecto 0). Se puede usar para mejorar convergencia.\n",
        "- centered:\tSi es True, usa una media m贸vil del gradiente para normalizar mejor. No se usa mucho.\n",
        "\n",
        "Funciona bien en problemas longitudinales como redes recurrentes (RNNs).\n",
        "No siempre converge a la mejor soluci贸n (a veces encuentra m铆nimos locales).\n",
        "Menos robusto en redes muy profundas comparado con Adam.\n",
        "\n",
        "\n",
        "## Adam\n",
        "\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)\n",
        "```\n",
        "\n",
        "- lr:\tTasa de aprendizaje (0.001 por defecto).\n",
        "- betas:\tFactores de decaimiento para momentum beta1 (0.9) y el segundo beta2 controla controla el beta del RMSProp\n",
        "- eps:\tT茅rmino de estabilidad (1e-8). Evita divisiones por cero.\n",
        "- weight_decay:\tRegularizaci贸n L2 (por defecto 0)\n",
        "\n",
        "Es el que mejor funciona porque combina todo lo anterior. Requiere mas memoria que los anteriores y m谩s c谩lculo. Hay que elegir sabiamente las betas para que vaya bien."
      ],
      "metadata": {
        "id": "Jh2u15C6KEOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicializacion de parametros\n",
        "\n",
        "Por defecto utiliza la siguiente inicializaci贸n y funciona muy bien, la verdad.\n",
        "\n",
        "\n",
        "| Capa | Inicializaci贸n por defecto |\n",
        "|------|----------------------------|\n",
        "|nn.Linear |\tUniforme en [-k, k], donde k = 1/in_features|\n",
        "| nn.Conv2d\t| Uniforme en [-k, k], donde k = 1/in_features|\n",
        "| nn.BatchNorm\t| pesos = 1, sesgos = 0 |\n",
        "\n",
        "Si se quiere utilizar las inicializaciones de Xavier, He y LeCun puede hacerse lo siguiente:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n0cYZIsgOgnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, X_nvars):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # Hidden Layer 1\n",
        "            torch.nn.Linear(X_nvars, 1), # Hacemos el sumatorio\n",
        "            torch.nn.ReLU(),              # Aplicamos la funci贸n de activacion\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(1, 1),\n",
        "        )\n",
        "\n",
        "        # Aqui es donde hacemos las incializaciones\n",
        "        for layer in self.layers:\n",
        "          if isinstance(layer, nn.Linear):\n",
        "            init.xavier_uniform_(layer.weight)# Si queremos xavier con uniforme\n",
        "            # init.xavier_normal_(layer.weight) # Si queremos xavier con normal\n",
        "\n",
        "            # init.kaiming_uniform_(layer.weight, nonlinearity='relu') # Si queremos He con uniforme\n",
        "            # init.kaiming_normal_(layer.weight, nonlinearity='relu') # Si queremos He con normal\n",
        "\n",
        "            # # LeCun normal\n",
        "            # init.normal_(layer.weight, mean=0, std=(1.0 / torch.sqrt(torch.tensor(layer.weight.size(1), dtype=torch.float))))\n",
        "\n",
        "            # # Inicializaci贸n LeCun uniforme (no hay una funci贸n directa, pero se puede hacer manualmente)\n",
        "            # fan_in = layer.weight.size(1)  # N煤mero de entradas\n",
        "            # bound = 1 / torch.sqrt(torch.tensor(fan_in, dtype=torch.float))\n",
        "            # init.uniform_(layer.weight, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layers(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "nmN_uIUSOkbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear conjuntos de train, dev y test\n",
        "\n",
        "Lo primero, hay que tener en cuenta estas dos funciones:\n",
        "\n",
        "```python\n",
        "model.train()\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "Ponen al modelo en modo entrenamiento o de evaluaci贸n y tiene efecto sobre algunas cosas del funcionamiento de la red, por ejemplo si se hacen o no los dropout, si se calcula el gradiente, etc.\n",
        "\n",
        "La divisi贸n entre los conjuntos de datos se puede hacer c贸mo quieras, hay m煤ltiples funciones para ello. Pytorch incorpora el random_split\n"
      ],
      "metadata": {
        "id": "KGRgO2mqRbzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GzIGiBQCRb7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "X = torch.randn(1000, 10)\n",
        "y = torch.randint(0, 2, (1000,))  # Clasificaci贸n binaria\n",
        "\n",
        "dataset = TensorDataset(X, y) # create your datset\n",
        "\n",
        "# Definir tama帽os de cada conjunto\n",
        "train_size = int(0.8 * len(dataset))  # 80% para entrenamiento\n",
        "dev_size = int(0.1 * len(dataset))  # 10% para validaci贸n\n",
        "test_size = len(dataset) - train_size - dev_size  # 10% para prueba\n",
        "\n",
        "# Dividir aleatoriamente\n",
        "train_dataset, dev_dataset, test_dataset = random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "# Crear dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dev_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "bmm5TWgiSkTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "\n",
        "Simplemente se introduce una capa de dropout despu茅s de la activaci贸n. El par谩metro p define la cantidad de datos que queremos perder.\n",
        "\n",
        "- Si lo aplicamos antes de la activaci贸n, algunas neuronas pueden ser eliminadas antes de calcular su valor 煤til, perdiendo informaci贸n antes de que sea procesada correctamente.\n",
        "- Aplicarlo despu茅s de la activaci贸n asegura que cada neurona ya tiene su contribuci贸n calculada antes de ser descartada temporalmente.\n"
      ],
      "metadata": {
        "id": "wIEtWqdiWBL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropout_p es la probabilidad de apagar una neurona (50%)\n",
        "dropout_p = 0.5\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, dropout_p):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Capa de entrada a capa oculta 1\n",
        "            nn.Linear(input_size, hidden_size_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),  # Dropout despu茅s de la capa oculta 1\n",
        "\n",
        "            # Capa oculta 1 a capa oculta 2\n",
        "            nn.Linear(hidden_size_1, hidden_size_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_p),  # Dropout despu茅s de la capa oculta 2\n",
        "\n",
        "            # Capa oculta 2 a capa de salida\n",
        "            nn.Linear(hidden_size_2, output_size),\n",
        "            nn.Sigmoid()  # Para clasificaci贸n binaria, puedes usar softmax para clasificaci贸n m煤ltiple\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "kzKQ4sxxWc--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch-normalization\n",
        "\n",
        "Batch Normalization es una t茅cnica para normalizar (media = 0 y sd = 1) las activaciones de cada capa en mini-batches. Esto ayuda a acelerar el entrenamiento y a estabilizar la red. En PyTorch, puedes usar torch.nn.BatchNorm1d, BatchNorm2d, o BatchNorm3d, dependiendo de la dimensi贸n de los datos que est茅s manejando.\n",
        "\n",
        "\n",
        "Ademas, la funci贸n BatchNorm de pytorch introduce dos nuevos par谩metros:\n",
        "\n",
        "- 尾 (bias): permite un desplazamiento en la media.\n",
        "- 纬 (scale factor): escala la distribuci贸n normalizada.\n",
        "\n",
        "Es decir, se le puede decir que en vez de acabar en (0,1) se reescale a y (尾,纬). Esto permite que la red aprenda si necesita reescalar o desplazar los valores."
      ],
      "metadata": {
        "id": "eJl2f7lGXOmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Capa de entrada a capa oculta 1\n",
        "            nn.Linear(input_size, hidden_size_1),\n",
        "            nn.BatchNorm1d(hidden_size_1),  # Normalizaci贸n despu茅s de la capa 1\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Capa oculta 1 a capa oculta 2\n",
        "            nn.Linear(hidden_size_1, hidden_size_2),\n",
        "            nn.BatchNorm1d(hidden_size_2),  # Normalizaci贸n despu茅s de la capa 2\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Capa oculta 2 a capa de salida\n",
        "            nn.Linear(hidden_size_2, output_size),\n",
        "            nn.Sigmoid()  # Para clasificaci贸n binaria\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Y0BPmlO4XeFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularizaciones L1 y L2\n",
        "\n",
        "Lo m谩s eficiente es usar L2 con el weight dacay de los optimizadores, pero si quieres programarla a pelo, o programar una elasticNet, puedes hacerlo:"
      ],
      "metadata": {
        "id": "8W_0j8ikXxLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define ElasticNet\n",
        "# Si haces l1_lambda o l2_lambda = 0 ese par谩metro no influye\n",
        "def l1_l2_regularization(model, l1_lambda, l2_lambda):\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters())  # L1\n",
        "    l2_norm = sum(p.pow(2).sum() for p in model.parameters())  # L2\n",
        "    return l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, targets) in enumerate(my_dataloader):\n",
        "\n",
        "        # forward\n",
        "        output = model(features)\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        # La regularizaci贸n\n",
        "        reg_loss = l1_l2_regularization(model, l1_lambda, l2_lambda)\n",
        "        total_loss = loss + reg_loss\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "d6pESIoF38CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usar la GPU\n",
        "\n",
        "Lo primero es crear un entorno de ejecuci贸n con una GPU\n",
        "\n",
        "Conectar -> Cambiar tipo de entorno de ejecuci贸n\n",
        "\n",
        "Y elegir GPU-T4"
      ],
      "metadata": {
        "id": "fPyEjat35LL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobamos si tenemos GPU\n",
        "import torch\n",
        "print(torch.cuda.is_available())  # Salida: True si hay una GPU disponible"
      ],
      "metadata": {
        "id": "XHOrUYwx5yHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si hay cuda indicamos que la GPU va a ser nuestro dispositivo de calculo, si no, la CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "NuqI1ZHv57gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si tentemos cuda ponemos imprimir informacion\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
      ],
      "metadata": {
        "id": "08HV6EfZ6TsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta funci贸n nos dice cosas de la Tarjeta grafica\n",
        "\n",
        "if device.type == 'cuda':\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "8KOEXY5l6Ym5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como lanzar un MLP sobre la GPU\n",
        "\n",
        "# Lo primero es pasar todos los tensores a la GPU\n",
        "\n",
        "# Dependiendo de la cantidad de memoria se recomiendan dos opciones:\n",
        "# A. Pasamos todos los datos a la GPU desde el principio (si tenemos mucha Vram)\n",
        "\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# DataLoader sin necesidad de mover lotes a GPU\n",
        "dataloader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n",
        "\n",
        "# B. Vamos pasando los mini-batch seg煤n los vayamos usando\n",
        "for epoch in range(5):\n",
        "    for x_batch, y_batch in dataloader:\n",
        "\n",
        "        # Mover solo el batch actual a GPU\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "\n",
        "# Lo segundo ejecutar el modelo sobre la GPU\n",
        "model = MLP().to(device)\n",
        "\n",
        "# Y el resto del c贸digo ser铆a igual que en otras ocasiones"
      ],
      "metadata": {
        "id": "RtxRTl6h6zPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B煤squeda de hiper-par谩metros con Optuna\n",
        "\n",
        "Vamos a partir de un MLP que ya vimos en la PRACTICA 04. Una red para clasificaci贸n binaria\n"
      ],
      "metadata": {
        "id": "fnvPVbgx-Ysj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "MSTruuRO-fGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos unos datos\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "import torcheval\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torcheval.metrics import BinaryAccuracy\n",
        "\n",
        "X = torch.randn(1000, 10)\n",
        "y = torch.randint(0, 2, (1000,))  # Clasificaci贸n binaria\n",
        "\n",
        "train_dataset = TensorDataset(X, y)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "x5nKVvMW-p0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el MLP\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.all_layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_features, 25),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(25, 1),\n",
        "            # torch.nn.Sigmoid() # ver comentario 1\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.all_layers(x)\n",
        "        return output.flatten()\n",
        "\n"
      ],
      "metadata": {
        "id": "qTxjtlpC_thD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define Optuna\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Aqui es donde se especifican los parametros que vamos a optimizar\n",
        "    # Vamos a optimizar solo el lr del optimizador\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.001, 0.01)\n",
        "\n",
        "    # Crear modelo con los hiperpar谩metros sugeridos\n",
        "    model = MLP(num_features = 10)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    accuracy = BinaryAccuracy()\n",
        "\n",
        "    model.train()\n",
        "    # Entrenamiento simple\n",
        "    for epoch in range(5):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = loss_fn(output, y_batch.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluar precisi贸n\n",
        "    model.eval()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        output = model(X_batch)\n",
        "        accuracy.update(output, y_batch)\n",
        "\n",
        "    return accuracy.compute()  # Optuna intentar谩 maximizar esto\n"
      ],
      "metadata": {
        "id": "vzMZMOSv_2Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")  # Buscamos maximizar la precisi贸n\n",
        "study.optimize(objective, n_trials=20)  # Probar 20 combinaciones\n",
        "\n",
        "#  Mostrar los mejores hiperpar谩metros encontrados\n",
        "print(\"Mejores hiperpar谩metros:\", study.best_params)"
      ],
      "metadata": {
        "id": "dm1YYOO2CVpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.matplotlib.plot_param_importances(study)"
      ],
      "metadata": {
        "id": "TSuHNbu8ImkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.matplotlib.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "rw1d4jZRIsPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio\n",
        "\n",
        "Para esta pr谩ctica vamos a utilizar el conjunto de datos MNIST que consiste en im谩genes de n煤meros del 0 al 9 escritas a mano y digitalizadas. Una descripci贸n del fichero la puedes encontrar en la [wikipedia](https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "Los datos est谩n ya almacenados en pytorch, con lo que con una 煤nica funci贸n los podemos cargar y, adem谩s, est谩n ya divididos en dos conjuntos de datos, un de entrenamiento con 60000 im谩genes y otro de test con 10000 im谩genes.\n",
        "\n",
        "El objetivo de la red es ser capaz de identificar correctamente el n煤mero y eso es equivalente a clasificar correctamente cada imagen.\n",
        "\n",
        "Los datos de entrada son im谩genes en escala de grises, una matriz bidimensional de 28 x 28 en la que cada pixel va de 0 a 255. La mejor manera de trabajar con im谩genes es a trav茅s de redes convolucionales pero todav铆a no las hemos estudiado as铆 que vamos a vectorizar la imagen, es decir, concatenar las columnas una tras otra y formas un unico vector para cada imagen.\n",
        "\n",
        "Para procesar los datos vamos a construir un perceptron multicapa como los de los ejercicios anteriores pero ahora como entrada vamos a tener\n",
        "\n",
        "- **Para el train:** 60000 im谩genes y sus etiquetas\n",
        "- **Para el test:** 10000 im谩genes y sus etiquetas\n"
      ],
      "metadata": {
        "id": "cN5HlhpaJELi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aEhsSamZKn7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(object):\n",
        "    def __call__(self, tensor):\n",
        "        # Aplanar la imagen a un vector de 784 p铆xeles\n",
        "        return tensor.view(-1)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convierte la imagen a tensor\n",
        "    transforms.Normalize((0.5,), (0.5,)),  # Normalizaci贸n\n",
        "    transforms.Lambda(lambda x: x.view(-1)) # Aplanar las im谩genes a un vector de 784 p铆xeles\n",
        "    # Flatten()  # Aplanar las im谩genes a un vector de 784 p铆xeles\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "tC9auyXdL6pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener una imagen y su etiqueta\n",
        "image, label = train_dataset[5]  # Obtener la primera imagen del conjunto de entrenamiento\n",
        "\n",
        "# Visualizar la imagen\n",
        "plt.imshow(image.view(28,28), cmap='gray')  # .squeeze() elimina la dimensi贸n de los canales (1,28,28 -> 28,28)\n",
        "plt.title(f\"Etiqueta: {label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qosa8eDoO5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 01\n",
        "\n",
        "Crea un MLP que realice una clasificaci贸n razonable, para ello utiliza las estrategias ***que consideres oportunas***.\n",
        "\n",
        "- Crear conjuntos de entrenamiento, dev y test\n",
        "- Ejecuta la red sobre una GPU\n",
        "- Buscar par谩metros con Optuna\n",
        "- Usar regularizaciones\n",
        "- etc."
      ],
      "metadata": {
        "id": "ZyJ76ryOJflC"
      }
    }
  ]
}