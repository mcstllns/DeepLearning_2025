{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVGhqwJypDY137PO6tsBwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning_2025/blob/main/PRACTICA_07_Trabajo_con_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"10\"><b>07. Trabajo con APIs</b></font>\n",
        "\n",
        "Miguel A. Castellanos\n",
        "\n",
        "\n",
        "\n",
        "**Contenidos:**\n",
        "\n",
        "<font color=\"darkorange\">\n",
        "\n",
        "1. Clasificación de textos\n",
        "  1. Clasificación Sentiment-Analysis\n",
        "  1. Clasificación zero-shot\n",
        "1. Gestión de los modelos\n",
        "1. Generación de Textos\n",
        "1. Respuestas a preguntas\n",
        "1. Resumir Textos\n",
        "1. Traducción\n",
        "1. ChatBot\n",
        "1. Embeddings\n",
        "1. Algunos datasets interesantes\n",
        "1. Cómo entrenar una red para una tarea específica (Fine-Tuning)\n",
        "\n",
        "</font>"
      ],
      "metadata": {
        "id": "Zsojn9UdhMc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una API (Application Programming Interface o Interfaz de Programación de Aplicaciones) es un conjunto de reglas y definiciones que permiten que diferentes aplicaciones o sistemas se comuniquen entre sí.\n",
        "\n",
        "Una API actúa como un intermediario que facilita la interacción entre diferentes programas o servicios. Funciona enviando solicitudes y recibiendo respuestas en formatos como JSON o XML.\n",
        "\n",
        "La API Transformers de Hugging Face es una biblioteca de código abierto en Python que permite el uso de modelos de procesamiento de lenguaje natural (NLP) y otros tipos de modelos de inteligencia artificial basados en redes neuronales profundas, como los Transformers.\n",
        "\n",
        "Facilita la implementación de modelos avanzados de IA, como GPT, BERT, T5, RoBERTa y muchos otros, para tareas como:\n",
        "\n",
        "- Análisis de sentimientos: Determina la polaridad de un texto, identificando si es positivo, negativo o neutral.​\n",
        "- Clasificación de texto: Asigna categorías predefinidas a fragmentos de texto, como etiquetar correos electrónicos como \"spam\" o \"no spam\".​\n",
        "- Reconocimiento de entidades nombradas (NER): Identifica y clasifica entidades como nombres de personas, organizaciones o lugares dentro de un texto.​\n",
        "- Respuesta a preguntas: Proporciona respuestas precisas basadas en un contexto textual dado.​\n",
        "- Rellenado de máscaras: Completa palabras faltantes en una oración con las opciones más probables.​\n",
        "- Resumir textos: Genera resúmenes concisos de documentos o artículos extensos.​\n",
        "- Traducción automática: Traduce texto de un idioma a otro.​\n",
        "- Generación de texto: Produce texto coherente y contextual a partir de una entrada inicial.\n",
        "\n",
        "Y otras muchas más tareas.\n",
        "\n",
        "\n",
        "ChatGPT no es una AGI pero lo simula porque tiene implementadas todas esas tareas y el usuario la percibe como unitaria.\n",
        "\n",
        "La API de HuggingFace incorpora la mayoría de los modelo open que existen como:\n",
        "\n",
        "- BERT (Bidirectional Encoder Representations from Transformers): Modelo preentrenado que captura el contexto de una palabra considerando tanto su izquierda como su derecha en una oración.​\n",
        "\n",
        "- GPT (Generative Pre-trained Transformer): Modelo de lenguaje generativo que puede producir texto coherente y relevante en función de una entrada dada.​\n",
        "\n",
        "- RoBERTa (Robustly Optimized BERT Approach): Una variante de BERT optimizada con más datos y técnicas de entrenamiento mejoradas.​\n",
        "\n",
        "- T5 (Text-To-Text Transfer Transformer): Modelo que convierte todas las tareas de procesamiento de texto en un formato de entrada-salida de texto a texto.​\n",
        "\n",
        "- DistilBERT: Versión reducida y más rápida de BERT que mantiene el 97% de su rendimiento con menos parámetros.​\n",
        "\n",
        "En total tiene unos 400.000 modelos, se pueden consultar en [https://huggingface.co/models](https://huggingface.co/models)\n",
        "\n",
        "\n",
        "Muchos de esos modelos han sido adaptados y entrenados al español.\n",
        "\n",
        "Consejo: no busques por castellano como palabra clave, busca por español o spanish, fuera de España a nadie le importa nuestro problemas idiomáticos.\n",
        "\n",
        "Vamos a ver las siguientes tareas, en cada una de ellas utilizaremos que, sin ser muy demandante (recuerda que colab es gratis y está limitado) permita realizar decentemente la tarea\n"
      ],
      "metadata": {
        "id": "fsdXa6HpyMHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En algunas tareas nos vamos a basar en el uso de pipelines, en otras, las más complejas, cargaremos y ejecutaremos el código de forma más básica\n",
        "\n",
        "En la biblioteca Transformers de Hugging Face, una pipeline es una herramienta que simplifica el uso de modelos preentrenados para realizar tareas específicas de procesamiento de lenguaje natural, visión por computadora y audio. Esta abstracción permite aplicar modelos complejos sin necesidad de profundizar en los detalles técnicos de preprocesamiento, tokenización o normalización de datos.\n",
        "\n",
        "\n",
        "Muchos de estos modelos, aunque son mínimos comparados con modelos \"avanzados\", solo se pueden usar a través de la GPU, por lo tanto arranca la T4 para poder realizar esta práctica.\n",
        "\n",
        "Si realizas todas las tareas de seguido, lo más probable es que al final Google te eche del entorno de ejecución porque has alcanzado la cuota gratis."
      ],
      "metadata": {
        "id": "cAEryHIKLQWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importante, vamos a lanzarlo todo sobre la GPU o será imposible ejecutar los modelos\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Verificar si la GPU está disponible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "zYGcPwiic8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opcional\n",
        "# puede ayudar a acelerar los calculos en HF pero también puede tardar bastante en instalarse\n",
        "# !pip install accelerate;"
      ],
      "metadata": {
        "cellView": "code",
        "id": "iTF9KEjp4tGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X5aG-7shLnM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo RoBERTa y asegurarse de que se ejecute en la GPU\n",
        "device = 0 if torch.cuda.is_available() else -1  # 0 para usar GPU, -1 para usar CPU\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>01. Clasificación de textos</b></font>"
      ],
      "metadata": {
        "id": "9ORMEEJY2m1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"5\"><b>Clasificación Sentiment-Analysis</b></font>\n",
        "\n",
        "\n",
        "Vamos a hacer un análisis de opiniones (Ojo, en inglés sentiment no es solo sentimientos). Se usa un modelo ya entrenado para ello en español.\n"
      ],
      "metadata": {
        "id": "3WINdVsZIKst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# Cargar tokenizador y modelo por separado\n",
        "model_name = \"pysentimiento/robertuito-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Crear pipeline usando el modelo y tokenizador ya cargados\n",
        "clasificador = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)"
      ],
      "metadata": {
        "id": "JavIW002JarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Textos para analizar\n",
        "textos = [\n",
        "    \"Estoy encantado con la compra, todo fue perfecto.\",\n",
        "    \"No me gustó el producto, esperaba algo mejor.\",\n",
        "    \"La atención fue aceptable, pero nada extraordinario.\"\n",
        "]\n",
        "\n",
        "# Realizar análisis de sentimientos\n",
        "resultados = clasificador(textos)\n",
        "\n",
        "# Mostrar resultados\n",
        "for texto, resultado in zip(textos, resultados):\n",
        "    print(f\"Texto: {texto}\\nSentimiento: {resultado['label']} ({resultado['score']:.3f})\\n\")\n"
      ],
      "metadata": {
        "id": "dkTFFTyAKeqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"5\"><b>Clasificación zero-shot</b></font>\n",
        "\n",
        "\n",
        "Zero shot se refiere a una tarea en la que el modelo no ha tenido entrenamiento previo con ejemplos específicos para esa tarea. Esto es, eneste caso una clasificacion para la que no ha sido entrenada la red.\n",
        "\n",
        "Una utilidad es la de clasificación de textos en función de eiquetas predefinidas.\n",
        "\n",
        "Obviamente la \"capacidad\" y el tamaño del modelo (los Billones) es determinante. Cuanto mayor sea el modelo más probable es que la realice bien sin entrenamiento previo.\n",
        "\n"
      ],
      "metadata": {
        "id": "cB8SHlvkDkFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar la pipeline de clasificación zero-shot\n",
        "\n",
        "model_name = \"facebook/bart-large-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "clasificador = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=device)"
      ],
      "metadata": {
        "id": "TyGcyT4zGDbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto a clasificar\n",
        "text = \"I love programming, it's my passion!\"\n",
        "\n",
        "# Etiquetas candidatas\n",
        "candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"love\", \"hate\", \"positive\", \"negative\"]\n",
        "\n",
        "# Clasificación\n",
        "result = clasificador(text, candidate_labels)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(result)"
      ],
      "metadata": {
        "id": "FtG9X8TmgOBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En castellano funciona razonablemente bien\n",
        "\n",
        "# Texto a clasificar\n",
        "text = \"Me gustan los helados de fresa\"\n",
        "\n",
        "# Etiquetas candidatas\n",
        "# candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"love\", \"hate\", \"positive\", \"negative\", \"summer\", \"winter\"]\n",
        "candidate_labels = [\"tecnologia\", \"deporte\", \"politica\", \"entretenimiento\", \"amor\", \"odio\", \"positivo\", \"negativo\", \"verano\", \"invierno\"]\n",
        "\n",
        "# Clasificación\n",
        "result = clasificador(text, candidate_labels)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ivC2dKNWgOEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>02. Gestión de los modelos</b></font>\n",
        "\n",
        "Si vamos descargando en disco y cargando en memoria modelos y modelos llegará un momento en el que la ejecución sea muy lenta o no podemos ejecutar nada, por eso es importante llevar una cierta \"higiene\" en el entorno de ejecución.\n",
        "\n",
        "A la derecha, en recursos, podemos ver la evolución de la carga de memoria (RAM y VRAM) y del disco duro\n",
        "\n",
        "Vamos a ver cómo eliminar de la memoria VRAM un modelo y luego eliminarlo del disco"
      ],
      "metadata": {
        "id": "p_ggL5V84nT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver la memoria RAM\n",
        "!free -h"
      ],
      "metadata": {
        "id": "hW1PYT_xNHvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver lo que nos dice la nvidia\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yqbYUnTTMo6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primero, liberar el modelo de la memoria de la GPU (VRAM)\n",
        "\n",
        "del model, tokenizer, clasificador\n",
        "# del model\n",
        "\n",
        "# Libera la memoria de la GPU\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Llama al colector de basura y elimina todo lo que vea que está suelto por ahí\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "eXpvvZD43cHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A veces despues de hacerlo se sigue quedando igual la memoria\n",
        "# esto es porque pytorch reserva memoria para futuras ejecuciones\n",
        "\n",
        "# Memoria asignada actualmente por tensores\n",
        "print(f\"Memoria asignada (GB): {torch.cuda.memory_allocated() / (1024 ** 3):.3f}\")\n",
        "\n",
        "# Memoria total reservada por PyTorch\n",
        "print(f\"Memoria reservada (GB): {torch.cuda.memory_reserved() / (1024 ** 3):.3f}\")\n"
      ],
      "metadata": {
        "id": "C2RJYOdfNlks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y hay veces que para eliminar del todo lo que hay en el VRAM hay que ir a Entorno de ejecución -> Reiniciar la sesión\n",
        "\n"
      ],
      "metadata": {
        "id": "iG434CWNOu_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instala el CLI de HF\n",
        "!pip install huggingface_hub[cli]"
      ],
      "metadata": {
        "id": "jEzowKxNFwhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos los modelos almacenados en cache\n",
        "!huggingface-cli scan-cache"
      ],
      "metadata": {
        "id": "4dwWWZFz4cxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import scan_cache_dir\n",
        "\n",
        "# Función de búsqueda del hash del modelo\n",
        "def busca_hash(modelo, cache_info):\n",
        "    for repo_info in cache_info.repos:\n",
        "        if repo_info.repo_id == modelo:\n",
        "            print(f\"Modelo: {repo_info.repo_id}\")\n",
        "            for revision in repo_info.revisions:\n",
        "                print(f\"  Hash del commit: {revision.commit_hash}\")\n",
        "                print(f\"  Referencias: {revision.refs}\")\n",
        "            return revision.commit_hash\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "cache_info = scan_cache_dir()\n",
        "hash = busca_hash(\"facebook/bart-large-mnli\", cache_info)\n",
        "\n",
        "if hash:\n",
        "    delete_strategy = cache_info.delete_revisions(hash)\n",
        "    # Mostrar el espacio que se liberará\n",
        "    print(f\"Se liberarán {delete_strategy.expected_freed_size_str}.\")\n",
        "    # Ejecutar la eliminación\n",
        "    delete_strategy.execute()\n",
        "else:\n",
        "    print(\"No se encontró el modelo especificado en la caché.\")"
      ],
      "metadata": {
        "id": "m5nty6V_F_W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del hash, delete_strategy"
      ],
      "metadata": {
        "id": "G-yBNWH76h6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si vuelves a ejecutar el scan-cache el modelo ha desaparecido\n",
        "!huggingface-cli scan-cache"
      ],
      "metadata": {
        "id": "cplJUc2IG7W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>03. Generación de Textos</b></font>"
      ],
      "metadata": {
        "id": "1DHfhAptHPUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"DeepESP/gpt2-spanish-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "oLflM34QH1ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada = \"En un futuro distópico, la inteligencia artificial ha\"\n",
        "inputs = tokenizer(entrada, return_tensors=\"pt\").to(device) # devuelve tensores de pytorch"
      ],
      "metadata": {
        "id": "7IiF3EH2H1lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parámetros del modelo\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_length=200,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "45h3uEf8H1om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los parametros son:\n",
        "\n",
        "\n",
        "| Parámetro                | Descripción                                                                    | Efecto                                |\n",
        "|--------------------------- |--------------------------------------------------------------------------------|---------------------------------------|\n",
        "| `max_length=200`           | Longitud máxima del texto generado (incluye texto inicial).  | Genera texto con máximo 200 tokens.   |\n",
        "| `num_return_sequences=1`   | Número de secuencias generadas.                              | Genera solo una secuencia.            |\n",
        "| `no_repeat_ngram_size=2`   | Evita repetir frases (n-gramas) en el texto generado.        | Evita repeticiones de bigramas.       |\n",
        "| `temperature=0.7`          | Controla la aleatoriedad (0 determinista, mayor a 1 muy creativo). | Equilibrio entre coherencia y diversidad. |\n",
        "| `top_p=0.9`                | Usa palabras cuya probabilidad acumulada alcanza este valor. | Usa palabras más probables hasta el 90%.|\n",
        "| `top_k=50`                 | Considera las 50 palabras más probables en cada paso.        | Mantiene calidad, evita palabras raras.|\n",
        "| `do_sample=True`           | Activa sampling probabilístico al seleccionar palabras.      | Genera textos diversos y variados.    |\n",
        "| `early_stopping=True`      | Finaliza la generación al alcanzar condiciones de parada.    | Detiene la generación automáticamente.|\n"
      ],
      "metadata": {
        "id": "jtdtJGzHS3zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(texto_generado)"
      ],
      "metadata": {
        "id": "GzJQNWw4H1r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada = \"los helados de fresa\"\n",
        "inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(\n",
        "      inputs.input_ids,\n",
        "      max_length=200,\n",
        "      num_return_sequences=1,\n",
        "      no_repeat_ngram_size=2,\n",
        "      temperature=0.3,\n",
        "      top_p=0.9,\n",
        "      top_k=50,\n",
        "      do_sample=True,\n",
        "      early_stopping=True\n",
        "  )\n",
        "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(texto_generado)"
      ],
      "metadata": {
        "id": "EtZkyxnsH1vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>04. Respuestas a preguntas</b></font>"
      ],
      "metadata": {
        "id": "zYiUroyPT1yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "\n",
        "# Cargar el mejor modelo disponible en español para QA\n",
        "modelo_nombre = \"mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
        "\n",
        "# Cargar modelo y tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_nombre)\n",
        "modelo = AutoModelForQuestionAnswering.from_pretrained(modelo_nombre).to(device)\n",
        "\n",
        "# Crear el pipeline de QA con el modelo cargado\n",
        "qa_pipeline = pipeline(\"question-answering\", model=modelo, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# Contexto y pregunta en español\n",
        "contexto = \"\"\"\n",
        "Lionel Messi es un futbolista argentino nacido en Rosario.\n",
        "Ha ganado siete Balones de Oro y es ampliamente considerado uno de los mejores jugadores de fútbol de todos los tiempos.\n",
        "Jugó muchos años en el FC Barcelona antes de transferirse al Paris Saint-Germain en 2021.\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "pregunta = \"¿Dónde juega Lionel Messi actualmente?\"\n",
        "\n",
        "# Realizar la predicción\n",
        "resultado = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar respuesta\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {resultado['answer']} (Confianza: {resultado['score']:.3f})\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "pregunta = \"¿Cuántos balones de oro tiene?\"\n",
        "\n",
        "# Realizar la predicción\n",
        "resultado = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar respuesta\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {resultado['answer']} (Confianza: {resultado['score']:.3f})\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "pregunta = \"¿Jugó en el Real Madrid?\"\n",
        "\n",
        "# Realizar la predicción\n",
        "resultado = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar respuesta\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {resultado['answer']} (Confianza: {resultado['score']:.3f})\")"
      ],
      "metadata": {
        "id": "prVaePY5Txdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>05. Resumir Textos</b></font>\n"
      ],
      "metadata": {
        "id": "JK13kIYNURdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, logging\n",
        "import torch\n",
        "\n",
        "# Modelo especializado para resúmenes en español\n",
        "modelo_nombre = \"mrm8488/bert2bert_shared-spanish-finetuned-summarization\"\n",
        "\n",
        "# Silenciar mensajes no críticos\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Cargar modelo y tokenizador por separado\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_nombre)\n",
        "modelo = AutoModelForSeq2SeqLM.from_pretrained(modelo_nombre).to(device)\n",
        "\n",
        "# Crear pipeline de resumen (corregido aquí)\n",
        "resumidor = pipeline(\"summarization\", model=modelo, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# Texto largo que deseas resumir\n",
        "texto_largo = \"\"\"\n",
        "OpenAI es una empresa dedicada a la investigación en inteligencia artificial (IA) con sede en San Francisco, California.\n",
        "Fue fundada en 2015 por Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, John Schulman y Wojciech Zaremba,\n",
        "con el objetivo de desarrollar tecnologías avanzadas de IA de manera segura y beneficiosa para toda la humanidad.\n",
        "OpenAI ha desarrollado tecnologías reconocidas mundialmente, como GPT-3 y ChatGPT, herramientas ampliamente usadas en diversos campos.\n",
        "\"\"\"\n",
        "\n",
        "# Realizar resumen\n",
        "resumen = resumidor(texto_largo, max_length=50, min_length=20, do_sample=False)\n",
        "\n",
        "# Mostrar resultado\n",
        "print(f\"Resumen: {resumen[0]['summary_text']}\")\n"
      ],
      "metadata": {
        "id": "4nAmWqveUpd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>06. Traducción</b></font>"
      ],
      "metadata": {
        "id": "g9YY1A82XMRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opcinal, acelera calculos\n",
        "# !pip install sentencepiece"
      ],
      "metadata": {
        "id": "AtKm41DkUpq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator_es_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "texto_espanol = \"Estoy aprendiendo a traducir textos usando inteligencia artificial.\"\n",
        "traduccion_ingles = translator_es_en(texto_espanol)[0]['translation_text']\n",
        "\n",
        "print(\"Español:\", texto_espanol)\n",
        "print(\"Inglés:\", traduccion_ingles)"
      ],
      "metadata": {
        "id": "_czLME3rUpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>07. ChatBot</b></font>"
      ],
      "metadata": {
        "id": "L0xTBjPFXmX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias necesarias\n",
        "# Instalar las cosas y descargar el modelo puede llevar bastante tiempo\n",
        "# Pero la aplicación queda muy chula con gradio\n",
        "\n",
        "!pip install -q transformers accelerate sentencepiece gradio\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gc\n",
        "\n",
        "class ChatbotEspanol:\n",
        "    def __init__(self):\n",
        "        # Verificar si hay GPU disponible en Colab\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "\n",
        "        # Cargar el modelo Gemma 2B adaptado para español\n",
        "        # Este es uno de los mejores modelos livianos disponibles para español que funcionará bien en Colab\n",
        "        self.model_name = \"LenguajeNaturalAI/leniachat-gemma-2b-v0\"\n",
        "\n",
        "        # Inicializar el tokenizer\n",
        "        print(\"Cargando tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Configuración para optimizar el uso de memoria en Colab\n",
        "        print(\"Cargando modelo...\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Inicializar el historial de conversación\n",
        "        self.history = []\n",
        "        print(\"¡Chatbot listo!\")\n",
        "\n",
        "    def generate_response(self, user_input, history):\n",
        "        # Convertir el historial de Gradio al formato que necesitamos\n",
        "        self.history = []\n",
        "        for human, assistant in history:\n",
        "            self.history.append({\"role\": \"user\", \"content\": human})\n",
        "            if assistant:  # Puede ser None en la primera interacción\n",
        "                self.history.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "\n",
        "        # Añadir el input actual del usuario\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Preparar la conversación formateada para el modelo\n",
        "        conversation = []\n",
        "        for message in self.history:\n",
        "            conversation.append({\"role\": message[\"role\"], \"content\": message[\"content\"]})\n",
        "\n",
        "        # Generar tokens con la plantilla de chat adecuada\n",
        "        messages = self.tokenizer.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Tokenizar la conversación\n",
        "        inputs = self.tokenizer(messages, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Generar respuesta\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Obtener la respuesta generada\n",
        "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Limpiar memoria CUDA para evitar fugas de memoria en Colab\n",
        "        if self.device == \"cuda\":\n",
        "            del inputs, outputs\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return response\n",
        "\n",
        "# Inicializar el chatbot\n",
        "chatbot = ChatbotEspanol()"
      ],
      "metadata": {
        "id": "G1FMuVKuYUmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la interfaz con Gradio\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"green\")) as demo:\n",
        "    gr.Markdown(\"# 🤖 Chatbot en Español con Hugging Face\")\n",
        "    gr.Markdown(\"Este chatbot utiliza el modelo mlabonne/gemma-2b-it-spanish para generar respuestas en español.\")\n",
        "\n",
        "    chatbot_interface = gr.ChatInterface(\n",
        "        chatbot.generate_response,\n",
        "        title=\"Conversación\",\n",
        "        examples=[\n",
        "            \"¿Cómo estás hoy?\",\n",
        "            \"Explícame el calentamiento global como si tuviera 10 años\",\n",
        "            \"¿Qué libros recomiendas para aprender sobre inteligencia artificial?\",\n",
        "            \"Escribe un poema corto sobre la primavera\",\n",
        "            \"Dame tres ideas para mejorar mi productividad\"\n",
        "        ]\n",
        "        # retry_btn=\"Reintentar\",\n",
        "        # undo_btn=\"Deshacer\",\n",
        "        # clear_btn=\"Limpiar\"\n",
        "    )\n",
        "\n",
        "# Lanzar la interfaz\n",
        "demo.launch(debug=False, share=True)"
      ],
      "metadata": {
        "id": "wCAK7EAgoS6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.close()"
      ],
      "metadata": {
        "id": "R-CmFALiYUqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>08. Embeddings</b></font>\n",
        "\n",
        "\n",
        "​Los embeddings son representaciones matemáticas que transforman datos complejos y de alta dimensión, como palabras, imágenes o sonidos, en vectores de números reales en un espacio de menor dimensión. Esta transformación facilita el procesamiento y análisis de la información, preservando relaciones y patrones significativos entre los datos."
      ],
      "metadata": {
        "id": "r5_14eErr-sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Verificar si hay GPU disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando el dispositivo: {device}\")\n",
        "\n",
        "# Función para aplicar Mean Pooling sobre las representaciones del modelo\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output.last_hidden_state  # Última capa oculta\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Frases en español a convertir en embeddings\n",
        "sentences = [\"¿Cómo está el clima hoy?\", \"¿Qué tiempo hace hoy?\", \"¿Cuál es la capital de España?\"]\n",
        "\n",
        "# Cargar el tokenizador y el modelo en la GPU\n",
        "model_name = \"jinaai/jina-embeddings-v2-base-es\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
        "\n",
        "# Tokenizar las frases y moverlas a la GPU\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Obtener embeddings con el modelo (sin calcular gradientes)\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# Aplicar Mean Pooling para obtener una única representación por frase\n",
        "embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
        "\n",
        "# Normalizar los embeddings (para cálculos de similitud)\n",
        "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "# Mostrar los embeddings generados\n",
        "print(\"Embeddings generados:\")\n",
        "print(embeddings)\n",
        "\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "TkbIQr52smOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "JBZAXINwsmRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix = torch.matmul(embeddings, embeddings.T)\n",
        "\n",
        "# Convertir a numpy para visualizar mejor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import ace_tools as tools\n",
        "\n",
        "similarity_matrix_np = similarity_matrix.cpu().numpy()\n",
        "\n",
        "# Crear un DataFrame para mostrar la matriz de similitud\n",
        "df_similarity = pd.DataFrame(similarity_matrix_np, index=sentences, columns=sentences)\n",
        "\n",
        "# Mostrar la matriz de similitud\n",
        "# tools.display_dataframe_to_user(name=\"Matriz de Similitud\", dataframe=df_similarity)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df_similarity, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Matriz de Similitud del Coseno\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hd2TkuoEsmUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>09. Algunos datasets interesantes</b></font>\n",
        "\n",
        "\n",
        "Aquí tienes un listado de algunos de los mejores datasets en español disponibles gratuitamente para trabajar con modelos de lenguaje (LLMs):\n",
        "\n",
        "1. MLSUM - Corpus de resúmenes de noticias en español con más de 250,000 artículos de El País.\n",
        "1. Spanish Billion Words Corpus - Colección masiva de textos en español de diversas fuentes web.\n",
        "1. MC4-es - Subset en español del Multilingual C4 de Google, con textos extraídos de Common Crawl.\n",
        "1. OPUS - Colección de textos paralelos traducidos en múltiples idiomas, incluyendo español.\n",
        "1. esWiki-abstracts - Abstracts de la Wikipedia en español.\n",
        "1. SQAC (Spanish Question Answering Corpus) - Dataset de preguntas y respuestas en español.\n",
        "1. XNLI-es - Versión en español del dataset de inferencia de lenguaje natural.\n",
        "1. PAWS-X-es - Corpus para evaluación de paráfrasis en español.\n",
        "1. HC3-Spanish - Dataset de conversaciones humano-chatbot en español para detectar texto generado.\n",
        "1. CardioSentiBr - Opiniones de pacientes sobre hospitales en español.\n",
        "1. TASS - Corpus de tweets en español con análisis de sentimiento.\n",
        "1. MeliSA - Dataset de opiniones de productos de Mercado Libre con análisis de sentimiento.\n",
        "1. CAPITEL - Corpus anotado de la Real Academia Española.\n",
        "1. Spanish SQuAD - Versión traducida del dataset SQuAD para respuesta a preguntas.\n",
        "1. MarIA datasets - Colección de datasets para el entrenamiento de modelos en español.\n",
        "\n",
        "1. Conjunto de Datos de Reseñas de Amazon\n",
        "Este dataset recopila reseñas de productos de Amazon en diversos idiomas, incluyendo el español. Es ampliamente utilizado para tareas de clasificación de texto y análisis de sentimientos.\n",
        "\n",
        "1. Conjunto de Datos de Twitter para Análisis de Sentimientos\n",
        "Existen múltiples datasets que recopilan tuits en español, etiquetados según el sentimiento expresado. Estos conjuntos de datos son valiosos para entrenar modelos en tareas de análisis de sentimientos y detección de emociones.\n",
        "\n",
        "1. Corpus de Mensajes de Foros y Redes Sociales\n",
        "Algunas universidades y centros de investigación han compilado conjuntos de datos que incluyen mensajes de foros y redes sociales en español, útiles para analizar interacciones y comportamientos en línea.\n",
        "\n",
        "1. Corpus de Noticias en Español\n",
        "Recopilaciones de artículos de noticias en español, provenientes de diversas fuentes, que pueden ser utilizadas para tareas de resumen automático, clasificación de noticias y detección de temas.\n",
        "\n",
        "1. Conjunto de Datos de Comentarios en Redes Sociales\n",
        "Algunas plataformas académicas ofrecen datasets que contienen comentarios y publicaciones de redes sociales en español, útiles para el análisis de opiniones y estudios sociológicos.\n",
        "\n",
        "1. Corpus de Textos Legales en Español\n",
        "Compilaciones de documentos legales y jurídicos en español, que pueden ser utilizadas para entrenar modelos especializados en el ámbito legal.\n",
        "\n",
        "1. Corpus de Conversaciones en Español\n",
        "Datasets que recopilan conversaciones y chats en español, útiles para entrenar modelos de diálogo y chatbots.\n",
        "\n",
        "\n",
        "La mayoría de estos datasets están disponibles a través de Hugging Face Datasets, el Linguistic Data Consortium o los repositorios de sus creadores. Para casos específicos como redes sociales, algunos investigadores han publicado datasets anonimizados que cumplen con los términos de servicio de las plataformas."
      ],
      "metadata": {
        "id": "z24kStUDvbbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además existe el paquete datasets de HuggingFace que incluye unas 300.000 bases, casi todas en inglés, aunque no solamente\n",
        "\n",
        "[https://huggingface.co/datasets?p=99&sort=trending](https://huggingface.co/datasets?p=99&sort=trending)"
      ],
      "metadata": {
        "id": "WwUgAv6UyxOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "m7So0MnByLe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset de reseñas de amazon (en inglés)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Cargar el conjunto de datos de reseñas de Amazon\n",
        "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n",
        "\n",
        "# Mostrar la primera reseña\n",
        "print(dataset[\"full\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61rp4cp9yLj8",
        "outputId": "ac395d7e-5123-47bd-a1d9-82c27bdb0b5c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rating': 5.0, 'title': 'Such a lovely scent but not overpowering.', 'text': \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\", 'images': [], 'asin': 'B00YQ6X8EO', 'parent_asin': 'B00YQ6X8EO', 'user_id': 'AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'timestamp': 1588687728923, 'helpful_vote': 0, 'verified_purchase': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset de amazon en español\n",
        "dataset = load_dataset(\"SetFit/amazon_reviews_multi_es\")"
      ],
      "metadata": {
        "id": "D_4vKr-H0qcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras 5 reseñas\n",
        "for i in range(5):\n",
        "    print(dataset['train'][i])"
      ],
      "metadata": {
        "id": "3g0HAFf-0O2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>10. Cómo entrenar un LLM para una tarea específica (Fine Tuning)</b></font>"
      ],
      "metadata": {
        "id": "kHJmW6-G5Q9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch"
      ],
      "metadata": {
        "id": "luoPCqJsLGvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Cargar el conjunto de datos de cuentos en español\n",
        "dataset = load_dataset(\"Fernandoefg/cuentos_es\")"
      ],
      "metadata": {
        "id": "ltBObaGPLO7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprime la estructura\n",
        "print(dataset)\n",
        "\n",
        "# ver cuento 0\n",
        "# print(dataset[\"train\"][0][\"content\"])\n",
        "\n",
        "# ver todos los titulos\n",
        "titulos = [cuento[\"title\"] for cuento in dataset[\"train\"]]\n",
        "print(titulos[:10])  # Muestra los primeros 10 títulos\n",
        "\n",
        "# buscar por un titulo\n",
        "# titulo_buscado = \"El Principito\"\n",
        "\n",
        "# cuento = next((cuento for cuento in dataset[\"train\"] if cuento[\"title\"] == titulo_buscado), None)\n",
        "\n",
        "# if cuento:\n",
        "#     print(f\"Título: {cuento['title']}\\nAutor: {cuento['author']}\\n\\n{cuento['content']}\")\n",
        "# else:\n",
        "#     print(\"No se encontró el cuento.\")"
      ],
      "metadata": {
        "id": "SsoBbAV2LfhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se tokeniza el texto y se añaden las etiquetas (labels)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Cargar el tokenizador de un modelo GPT-2 en español\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
        "\n",
        "# Tokenizar el texto de las historias y añadir labels\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(examples[\"content\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # Se necesita que labels sea igual a input_ids para la pérdida de entrenamiento\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Aplicar la tokenización al conjunto de datos\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "u0-C5Ap0L2WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Como el conjunto no tiene explicito ttrain y test hacemos un split\n",
        "dataset_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "# Crear un nuevo DatasetDict con train y test\n",
        "midataset = DatasetDict({\n",
        "    \"train\": dataset_split[\"train\"],\n",
        "    \"test\": dataset_split[\"test\"]\n",
        "})"
      ],
      "metadata": {
        "id": "sfN583MuPoOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Cargar el modelo GPT-2 en español para generación de texto\n",
        "model = AutoModelForCausalLM.from_pretrained(\"datificate/gpt2-small-spanish\")"
      ],
      "metadata": {
        "id": "6yqPUkeMND-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "output_dir=\"./resultados_historias\",  # Carpeta donde se guardan los resultados\n",
        "    run_name=\"entrenamiento_historias\",  # Nombre único para el experimento en WandB\n",
        "    report_to=\"none\",  # Desactiva WandB si no lo necesitas\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")"
      ],
      "metadata": {
        "id": "sClCHQzHNIKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=midataset[\"train\"],\n",
        "    eval_dataset=midataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "_wLsy0XkNK9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La tarea es tan demandante que no se puede llevar a cabo en colab\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3zI5tKegNNVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo ajustado\n",
        "from transformers import pipeline\n",
        "\n",
        "modelo_ajustado = pipeline(\"text-generation\", model=\"./resultados_historias\")\n",
        "\n",
        "# Generar una historia a partir de una frase inicial\n",
        "prompt = \"Érase una vez en un bosque encantado,\"\n",
        "resultados = modelo_ajustado(prompt, max_length=200, num_return_sequences=1)\n",
        "\n",
        "print(resultados[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "OgIHjim7NP7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se puede ver las posibilidades son infinitas, aquí dejo una lista de tipos de entrenamiento para los que las librerías de huggingFace están diseñadas:\n",
        "\n",
        "## Tareas de NLP para Modelos Preentrenados\n",
        "\n",
        "1. **Clasificación de Texto**\n",
        "   - **Descripción:** Asignar categorías predefinidas a fragmentos de texto.\n",
        "   - **Ejemplos:** Clasificación de correos electrónicos como \"spam\" o \"no spam\", análisis de sentimientos en reseñas de productos.\n",
        "\n",
        "2. **Reconocimiento de Entidades Nombradas (NER)**\n",
        "   - **Descripción:** Identificar y clasificar entidades como nombres de personas, organizaciones o lugares dentro de un texto.\n",
        "   - **Ejemplos:** En la frase \"Apple lanzó el nuevo iPhone en California\", reconocer \"Apple\" como organización y \"California\" como lugar.\n",
        "\n",
        "3. **Análisis de Sentimientos**\n",
        "   - **Descripción:** Determinar la polaridad emocional de un texto, como positiva, negativa o neutral.\n",
        "   - **Ejemplos:** Analizar opiniones en redes sociales para evaluar la percepción pública de una marca.\n",
        "\n",
        "4. **Traducción Automática**\n",
        "   - **Descripción:** Convertir texto de un idioma a otro.\n",
        "   - **Ejemplos:** Traducir documentos del inglés al español.\n",
        "\n",
        "5. **Resumen de Texto**\n",
        "   - **Descripción:** Generar una versión condensada de un texto más largo, manteniendo la información esencial.\n",
        "   - **Ejemplos:** Resumir artículos de noticias o informes extensos.\n",
        "\n",
        "6. **Respuesta a Preguntas (QA)**\n",
        "   - **Descripción:** Proporcionar respuestas precisas a preguntas formuladas en lenguaje natural, basándose en un contexto dado.\n",
        "   - **Ejemplos:** Responder \"¿Quién es el presidente de Francia?\" utilizando una base de datos de conocimiento.\n",
        "\n",
        "7. **Análisis de Sintaxis y Dependencias**\n",
        "   - **Descripción:** Descomponer oraciones para entender su estructura gramatical y las relaciones entre palabras.\n",
        "   - **Ejemplos:** Identificar el sujeto, verbo y objeto en una oración.\n",
        "\n",
        "8. **Detección de Lenguaje**\n",
        "   - **Descripción:** Identificar el idioma en el que está escrito un texto.\n",
        "   - **Ejemplos:** Determinar que un documento está en francés.\n",
        "\n",
        "9. **Reconocimiento de Voz a Texto**\n",
        "   - **Descripción:** Transcribir contenido hablado en texto escrito.\n",
        "   - **Ejemplos:** Convertir grabaciones de entrevistas en texto para análisis.\n",
        "\n",
        "10. **Conversión de Texto a Voz**\n",
        "    - **Descripción:** Generar audio hablado a partir de texto escrito.\n",
        "    - **Ejemplos:** Lectores de pantalla para personas con discapacidad visual.\n",
        "\n",
        "11. **Detección de Parafraseo**\n",
        "    - **Descripción:** Identificar si dos frases tienen el mismo significado.\n",
        "    - **Ejemplos:** Detectar duplicados en respuestas de encuestas.\n",
        "\n",
        "12. **Resolución de Coreferencias**\n",
        "    - **Descripción:** Determinar qué palabras en un texto se refieren a la misma entidad.\n",
        "    - **Ejemplos:** En \"María dijo que ella vendría\", identificar que \"ella\" se refiere a \"María\".\n",
        "\n",
        "13. **Extracción de Información**\n",
        "    - **Descripción:** Extraer datos estructurados específicos de textos no estructurados.\n",
        "    - **Ejemplos:** Obtener fechas y lugares de eventos mencionados en artículos de noticias.\n",
        "\n",
        "14. **Corrección Gramatical**\n",
        "    - **Descripción:** Identificar y corregir errores gramaticales en un texto.\n",
        "    - **Ejemplos:** Sugerir correcciones en redacciones estudiantiles.\n",
        "\n",
        "15. **Desambiguación de Palabras**\n",
        "    - **Descripción:** Determinar el significado correcto de una palabra que tiene múltiples sentidos, según el contexto.\n",
        "    - **Ejemplos:** En \"El banco está cerrado\", decidir si \"banco\" se refiere a una entidad financiera o a un asiento.\n",
        "\n",
        "16. **Generación de Código**\n",
        "    - **Descripción:** Escribir código de programación basado en descripciones en lenguaje natural.\n",
        "    - **Ejemplos:** Generar una función en Python que calcule la suma de una lista de números.\n",
        "\n",
        "17. **Conversión de Código a Lenguaje Natural**\n",
        "    - **Descripción:** Explicar en lenguaje natural lo que hace un fragmento de código.\n",
        "    - **Ejemplos:** Describir la funcionalidad de una función en JavaScript.\n",
        "\n",
        "18. **Detección de Humor o Sarcasmo**\n",
        "    - **Descripción:** Identificar tonos humorísticos o sarcásticos en textos.\n",
        "    - **Ejemplos:** Analizar tweets para detectar sarcasmo.\n",
        "\n",
        "19. **Análisis de Temas**\n",
        "    - **Descripción:** Identificar temas o tópicos predominantes en un conjunto de documentos.\n",
        "    - **Ejemplos:** Descubrir que un conjunto de artículos de noticias trata principalmente sobre economía y salud.\n",
        "\n",
        "20. **Generación de Preguntas**\n",
        "    - **Descripción:** Crear preguntas relevantes basadas en un texto dado.\n",
        "    - **Ejemplos:** Generar preguntas de comprensión lectora para un párrafo educativo.\n",
        "\n",
        "21. **Conversión de Formato de Texto**\n",
        "    - **Descripción:** Transformar texto entre diferentes formatos o estilos.\n",
        "    - **Ejemplos:** Convertir texto plano en formato JSON o XML.\n",
        "\n",
        "22. **Detección de Plagio**\n",
        "    - **Descripción:** Identificar contenido copiado o similar a otros textos existentes.\n",
        "    - **Ejemplos:** Verificar la originalidad de trabajos académicos.\n",
        "\n",
        "23. **Análisis de Redes Sociales**\n",
        "    - **Descripción:** Analizar contenido de plataformas sociales para obtener insights.\n",
        "    - **Ejemplos:** Evaluar la reacción del público ante un evento reciente.\n",
        "\n",
        "24. **Clasificación de Intenciones**\n",
        "    - **Descripción:** Identificar la intención detrás de una consulta o mensaje.\n",
        "    - **Ejemplos:** Determinar si una pregunta en un chatbot es una consulta de información o una solicitud de acción.\n",
        "\n",
        "25. **Detección de Entidades de Producto**\n",
        "    - **Descripción:** Identificar menciones de productos específicos en textos.\n",
        "    - **Ejemplos:** Encontrar referencias a modelos de teléfonos en reseñas.\n",
        "\n",
        "26. **Análisis de Competencia**\n",
        "    - **Descripción:** Evaluar menciones y sentimientos hacia competidores en el mercado.\n",
        "    - **Ejemplos:** Analizar cómo se perciben diferentes marcas en comentarios de clientes.\n",
        "\n",
        "27. **Detección de Noticias Falsas**\n",
        "    - **Descripción:** Identificar la veracidad de la información presentada en un texto.\n",
        "    - **Ejemplos:** Evaluar si un artículo de noticias contiene información falsa o engañosa.\n",
        "\n",
        "28. **Detección de Contenido Ofensivo**\n",
        "    - **Descripción:** Identificar lenguaje inapropiado o dañino en textos.\n",
        "    - **Ejemplos:** Filtrar comentarios ofensivos en plataformas en línea.\n",
        "\n",
        "29. **Análisis de Discurso**\n",
        "    - **Descripción:** Examinar textos para entender estructuras argumentativas o retóricas.\n",
        "    - **Ejemplos:** Analizar discursos políticos para identificar argumentos clave.\n",
        "\n",
        "30. **Generación de Descripciones de Imágenes**\n",
        "    - **Descripción:** Crear descripciones textuales basadas en el contenido de imágenes.\n",
        "    - **Ejemplos:** Describir el contenido de una fotografía\n"
      ],
      "metadata": {
        "id": "HnIQhccBT6CV"
      }
    }
  ]
}