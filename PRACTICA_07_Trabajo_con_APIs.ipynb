{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVGhqwJypDY137PO6tsBwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/DeepLearning_2025/blob/main/PRACTICA_07_Trabajo_con_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"10\"><b>07. Trabajo con APIs</b></font>\n",
        "\n",
        "Miguel A. Castellanos\n",
        "\n",
        "\n",
        "\n",
        "**Contenidos:**\n",
        "\n",
        "<font color=\"darkorange\">\n",
        "\n",
        "1. Clasificaci√≥n de textos\n",
        "  1. Clasificaci√≥n Sentiment-Analysis\n",
        "  1. Clasificaci√≥n zero-shot\n",
        "1. Gesti√≥n de los modelos\n",
        "1. Generaci√≥n de Textos\n",
        "1. Respuestas a preguntas\n",
        "1. Resumir Textos\n",
        "1. Traducci√≥n\n",
        "1. ChatBot\n",
        "1. Embeddings\n",
        "1. Algunos datasets interesantes\n",
        "1. C√≥mo entrenar una red para una tarea espec√≠fica (Fine-Tuning)\n",
        "\n",
        "</font>"
      ],
      "metadata": {
        "id": "Zsojn9UdhMc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una API (Application Programming Interface o Interfaz de Programaci√≥n de Aplicaciones) es un conjunto de reglas y definiciones que permiten que diferentes aplicaciones o sistemas se comuniquen entre s√≠.\n",
        "\n",
        "Una API act√∫a como un intermediario que facilita la interacci√≥n entre diferentes programas o servicios. Funciona enviando solicitudes y recibiendo respuestas en formatos como JSON o XML.\n",
        "\n",
        "La API Transformers de Hugging Face es una biblioteca de c√≥digo abierto en Python que permite el uso de modelos de procesamiento de lenguaje natural (NLP) y otros tipos de modelos de inteligencia artificial basados en redes neuronales profundas, como los Transformers.\n",
        "\n",
        "Facilita la implementaci√≥n de modelos avanzados de IA, como GPT, BERT, T5, RoBERTa y muchos otros, para tareas como:\n",
        "\n",
        "- An√°lisis de sentimientos: Determina la polaridad de un texto, identificando si es positivo, negativo o neutral.‚Äã\n",
        "- Clasificaci√≥n de texto: Asigna categor√≠as predefinidas a fragmentos de texto, como etiquetar correos electr√≥nicos como \"spam\" o \"no spam\".‚Äã\n",
        "- Reconocimiento de entidades nombradas (NER): Identifica y clasifica entidades como nombres de personas, organizaciones o lugares dentro de un texto.‚Äã\n",
        "- Respuesta a preguntas: Proporciona respuestas precisas basadas en un contexto textual dado.‚Äã\n",
        "- Rellenado de m√°scaras: Completa palabras faltantes en una oraci√≥n con las opciones m√°s probables.‚Äã\n",
        "- Resumir textos: Genera res√∫menes concisos de documentos o art√≠culos extensos.‚Äã\n",
        "- Traducci√≥n autom√°tica: Traduce texto de un idioma a otro.‚Äã\n",
        "- Generaci√≥n de texto: Produce texto coherente y contextual a partir de una entrada inicial.\n",
        "\n",
        "Y otras muchas m√°s tareas.\n",
        "\n",
        "\n",
        "ChatGPT no es una AGI pero lo simula porque tiene implementadas todas esas tareas y el usuario la percibe como unitaria.\n",
        "\n",
        "La API de HuggingFace incorpora la mayor√≠a de los modelo open que existen como:\n",
        "\n",
        "- BERT (Bidirectional Encoder Representations from Transformers): Modelo preentrenado que captura el contexto de una palabra considerando tanto su izquierda como su derecha en una oraci√≥n.‚Äã\n",
        "\n",
        "- GPT (Generative Pre-trained Transformer): Modelo de lenguaje generativo que puede producir texto coherente y relevante en funci√≥n de una entrada dada.‚Äã\n",
        "\n",
        "- RoBERTa (Robustly Optimized BERT Approach): Una variante de BERT optimizada con m√°s datos y t√©cnicas de entrenamiento mejoradas.‚Äã\n",
        "\n",
        "- T5 (Text-To-Text Transfer Transformer): Modelo que convierte todas las tareas de procesamiento de texto en un formato de entrada-salida de texto a texto.‚Äã\n",
        "\n",
        "- DistilBERT: Versi√≥n reducida y m√°s r√°pida de BERT que mantiene el 97% de su rendimiento con menos par√°metros.‚Äã\n",
        "\n",
        "En total tiene unos 400.000 modelos, se pueden consultar en [https://huggingface.co/models](https://huggingface.co/models)\n",
        "\n",
        "\n",
        "Muchos de esos modelos han sido adaptados y entrenados al espa√±ol.\n",
        "\n",
        "Consejo: no busques por castellano como palabra clave, busca por espa√±ol o spanish, fuera de Espa√±a a nadie le importa nuestro problemas idiom√°ticos.\n",
        "\n",
        "Vamos a ver las siguientes tareas, en cada una de ellas utilizaremos que, sin ser muy demandante (recuerda que colab es gratis y est√° limitado) permita realizar decentemente la tarea\n"
      ],
      "metadata": {
        "id": "fsdXa6HpyMHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En algunas tareas nos vamos a basar en el uso de pipelines, en otras, las m√°s complejas, cargaremos y ejecutaremos el c√≥digo de forma m√°s b√°sica\n",
        "\n",
        "En la biblioteca Transformers de Hugging Face, una pipeline es una herramienta que simplifica el uso de modelos preentrenados para realizar tareas espec√≠ficas de procesamiento de lenguaje natural, visi√≥n por computadora y audio. Esta abstracci√≥n permite aplicar modelos complejos sin necesidad de profundizar en los detalles t√©cnicos de preprocesamiento, tokenizaci√≥n o normalizaci√≥n de datos.\n",
        "\n",
        "\n",
        "Muchos de estos modelos, aunque son m√≠nimos comparados con modelos \"avanzados\", solo se pueden usar a trav√©s de la GPU, por lo tanto arranca la T4 para poder realizar esta pr√°ctica.\n",
        "\n",
        "Si realizas todas las tareas de seguido, lo m√°s probable es que al final Google te eche del entorno de ejecuci√≥n porque has alcanzado la cuota gratis."
      ],
      "metadata": {
        "id": "cAEryHIKLQWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importante, vamos a lanzarlo todo sobre la GPU o ser√° imposible ejecutar los modelos\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Verificar si la GPU est√° disponible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "zYGcPwiic8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opcional\n",
        "# puede ayudar a acelerar los calculos en HF pero tambi√©n puede tardar bastante en instalarse\n",
        "# !pip install accelerate;"
      ],
      "metadata": {
        "cellView": "code",
        "id": "iTF9KEjp4tGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X5aG-7shLnM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo RoBERTa y asegurarse de que se ejecute en la GPU\n",
        "device = 0 if torch.cuda.is_available() else -1  # 0 para usar GPU, -1 para usar CPU\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>01. Clasificaci√≥n de textos</b></font>"
      ],
      "metadata": {
        "id": "9ORMEEJY2m1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"5\"><b>Clasificaci√≥n Sentiment-Analysis</b></font>\n",
        "\n",
        "\n",
        "Vamos a hacer un an√°lisis de opiniones (Ojo, en ingl√©s sentiment no es solo sentimientos). Se usa un modelo ya entrenado para ello en espa√±ol.\n"
      ],
      "metadata": {
        "id": "3WINdVsZIKst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# Cargar tokenizador y modelo por separado\n",
        "model_name = \"pysentimiento/robertuito-sentiment-analysis\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Crear pipeline usando el modelo y tokenizador ya cargados\n",
        "clasificador = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)"
      ],
      "metadata": {
        "id": "JavIW002JarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Textos para analizar\n",
        "textos = [\n",
        "    \"Estoy encantado con la compra, todo fue perfecto.\",\n",
        "    \"No me gust√≥ el producto, esperaba algo mejor.\",\n",
        "    \"La atenci√≥n fue aceptable, pero nada extraordinario.\"\n",
        "]\n",
        "\n",
        "# Realizar an√°lisis de sentimientos\n",
        "resultados = clasificador(textos)\n",
        "\n",
        "# Mostrar resultados\n",
        "for texto, resultado in zip(textos, resultados):\n",
        "    print(f\"Texto: {texto}\\nSentimiento: {resultado['label']} ({resultado['score']:.3f})\\n\")\n"
      ],
      "metadata": {
        "id": "dkTFFTyAKeqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"5\"><b>Clasificaci√≥n zero-shot</b></font>\n",
        "\n",
        "\n",
        "Zero shot se refiere a una tarea en la que el modelo no ha tenido entrenamiento previo con ejemplos espec√≠ficos para esa tarea. Esto es, eneste caso una clasificacion para la que no ha sido entrenada la red.\n",
        "\n",
        "Una utilidad es la de clasificaci√≥n de textos en funci√≥n de eiquetas predefinidas.\n",
        "\n",
        "Obviamente la \"capacidad\" y el tama√±o del modelo (los Billones) es determinante. Cuanto mayor sea el modelo m√°s probable es que la realice bien sin entrenamiento previo.\n",
        "\n"
      ],
      "metadata": {
        "id": "cB8SHlvkDkFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar la pipeline de clasificaci√≥n zero-shot\n",
        "\n",
        "model_name = \"facebook/bart-large-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "clasificador = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=device)"
      ],
      "metadata": {
        "id": "TyGcyT4zGDbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto a clasificar\n",
        "text = \"I love programming, it's my passion!\"\n",
        "\n",
        "# Etiquetas candidatas\n",
        "candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"love\", \"hate\", \"positive\", \"negative\"]\n",
        "\n",
        "# Clasificaci√≥n\n",
        "result = clasificador(text, candidate_labels)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(result)"
      ],
      "metadata": {
        "id": "FtG9X8TmgOBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En castellano funciona razonablemente bien\n",
        "\n",
        "# Texto a clasificar\n",
        "text = \"Me gustan los helados de fresa\"\n",
        "\n",
        "# Etiquetas candidatas\n",
        "# candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"love\", \"hate\", \"positive\", \"negative\", \"summer\", \"winter\"]\n",
        "candidate_labels = [\"tecnologia\", \"deporte\", \"politica\", \"entretenimiento\", \"amor\", \"odio\", \"positivo\", \"negativo\", \"verano\", \"invierno\"]\n",
        "\n",
        "# Clasificaci√≥n\n",
        "result = clasificador(text, candidate_labels)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ivC2dKNWgOEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>02. Gesti√≥n de los modelos</b></font>\n",
        "\n",
        "Si vamos descargando en disco y cargando en memoria modelos y modelos llegar√° un momento en el que la ejecuci√≥n sea muy lenta o no podemos ejecutar nada, por eso es importante llevar una cierta \"higiene\" en el entorno de ejecuci√≥n.\n",
        "\n",
        "A la derecha, en recursos, podemos ver la evoluci√≥n de la carga de memoria (RAM y VRAM) y del disco duro\n",
        "\n",
        "Vamos a ver c√≥mo eliminar de la memoria VRAM un modelo y luego eliminarlo del disco"
      ],
      "metadata": {
        "id": "p_ggL5V84nT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver la memoria RAM\n",
        "!free -h"
      ],
      "metadata": {
        "id": "hW1PYT_xNHvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver lo que nos dice la nvidia\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yqbYUnTTMo6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# primero, liberar el modelo de la memoria de la GPU (VRAM)\n",
        "\n",
        "del model, tokenizer, clasificador\n",
        "# del model\n",
        "\n",
        "# Libera la memoria de la GPU\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Llama al colector de basura y elimina todo lo que vea que est√° suelto por ah√≠\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "eXpvvZD43cHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A veces despues de hacerlo se sigue quedando igual la memoria\n",
        "# esto es porque pytorch reserva memoria para futuras ejecuciones\n",
        "\n",
        "# Memoria asignada actualmente por tensores\n",
        "print(f\"Memoria asignada (GB): {torch.cuda.memory_allocated() / (1024 ** 3):.3f}\")\n",
        "\n",
        "# Memoria total reservada por PyTorch\n",
        "print(f\"Memoria reservada (GB): {torch.cuda.memory_reserved() / (1024 ** 3):.3f}\")\n"
      ],
      "metadata": {
        "id": "C2RJYOdfNlks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y hay veces que para eliminar del todo lo que hay en el VRAM hay que ir a Entorno de ejecuci√≥n -> Reiniciar la sesi√≥n\n",
        "\n"
      ],
      "metadata": {
        "id": "iG434CWNOu_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se instala el CLI de HF\n",
        "!pip install huggingface_hub[cli]"
      ],
      "metadata": {
        "id": "jEzowKxNFwhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos los modelos almacenados en cache\n",
        "!huggingface-cli scan-cache"
      ],
      "metadata": {
        "id": "4dwWWZFz4cxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import scan_cache_dir\n",
        "\n",
        "# Funci√≥n de b√∫squeda del hash del modelo\n",
        "def busca_hash(modelo, cache_info):\n",
        "    for repo_info in cache_info.repos:\n",
        "        if repo_info.repo_id == modelo:\n",
        "            print(f\"Modelo: {repo_info.repo_id}\")\n",
        "            for revision in repo_info.revisions:\n",
        "                print(f\"  Hash del commit: {revision.commit_hash}\")\n",
        "                print(f\"  Referencias: {revision.refs}\")\n",
        "            return revision.commit_hash\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "cache_info = scan_cache_dir()\n",
        "hash = busca_hash(\"facebook/bart-large-mnli\", cache_info)\n",
        "\n",
        "if hash:\n",
        "    delete_strategy = cache_info.delete_revisions(hash)\n",
        "    # Mostrar el espacio que se liberar√°\n",
        "    print(f\"Se liberar√°n {delete_strategy.expected_freed_size_str}.\")\n",
        "    # Ejecutar la eliminaci√≥n\n",
        "    delete_strategy.execute()\n",
        "else:\n",
        "    print(\"No se encontr√≥ el modelo especificado en la cach√©.\")"
      ],
      "metadata": {
        "id": "m5nty6V_F_W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del hash, delete_strategy"
      ],
      "metadata": {
        "id": "G-yBNWH76h6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Si vuelves a ejecutar el scan-cache el modelo ha desaparecido\n",
        "!huggingface-cli scan-cache"
      ],
      "metadata": {
        "id": "cplJUc2IG7W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>03. Generaci√≥n de Textos</b></font>"
      ],
      "metadata": {
        "id": "1DHfhAptHPUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"DeepESP/gpt2-spanish-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "oLflM34QH1ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada = \"En un futuro dist√≥pico, la inteligencia artificial ha\"\n",
        "inputs = tokenizer(entrada, return_tensors=\"pt\").to(device) # devuelve tensores de pytorch"
      ],
      "metadata": {
        "id": "7IiF3EH2H1lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# par√°metros del modelo\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_length=200,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "45h3uEf8H1om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los parametros son:\n",
        "\n",
        "\n",
        "| Par√°metro                | Descripci√≥n                                                                    | Efecto                                |\n",
        "|--------------------------- |--------------------------------------------------------------------------------|---------------------------------------|\n",
        "| `max_length=200`           | Longitud m√°xima del texto generado (incluye texto inicial).  | Genera texto con m√°ximo 200 tokens.   |\n",
        "| `num_return_sequences=1`   | N√∫mero de secuencias generadas.                              | Genera solo una secuencia.            |\n",
        "| `no_repeat_ngram_size=2`   | Evita repetir frases (n-gramas) en el texto generado.        | Evita repeticiones de bigramas.       |\n",
        "| `temperature=0.7`          | Controla la aleatoriedad (0 determinista, mayor a 1 muy creativo). | Equilibrio entre coherencia y diversidad. |\n",
        "| `top_p=0.9`                | Usa palabras cuya probabilidad acumulada alcanza este valor. | Usa palabras m√°s probables hasta el 90%.|\n",
        "| `top_k=50`                 | Considera las 50 palabras m√°s probables en cada paso.        | Mantiene calidad, evita palabras raras.|\n",
        "| `do_sample=True`           | Activa sampling probabil√≠stico al seleccionar palabras.      | Genera textos diversos y variados.    |\n",
        "| `early_stopping=True`      | Finaliza la generaci√≥n al alcanzar condiciones de parada.    | Detiene la generaci√≥n autom√°ticamente.|\n"
      ],
      "metadata": {
        "id": "jtdtJGzHS3zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(texto_generado)"
      ],
      "metadata": {
        "id": "GzJQNWw4H1r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada = \"los helados de fresa\"\n",
        "inputs = tokenizer(entrada, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(\n",
        "      inputs.input_ids,\n",
        "      max_length=200,\n",
        "      num_return_sequences=1,\n",
        "      no_repeat_ngram_size=2,\n",
        "      temperature=0.3,\n",
        "      top_p=0.9,\n",
        "      top_k=50,\n",
        "      do_sample=True,\n",
        "      early_stopping=True\n",
        "  )\n",
        "texto_generado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(texto_generado)"
      ],
      "metadata": {
        "id": "EtZkyxnsH1vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>04. Respuestas a preguntas</b></font>"
      ],
      "metadata": {
        "id": "zYiUroyPT1yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "\n",
        "# Cargar el mejor modelo disponible en espa√±ol para QA\n",
        "modelo_nombre = \"mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
        "\n",
        "# Cargar modelo y tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_nombre)\n",
        "modelo = AutoModelForQuestionAnswering.from_pretrained(modelo_nombre).to(device)\n",
        "\n",
        "# Crear el pipeline de QA con el modelo cargado\n",
        "qa_pipeline = pipeline(\"question-answering\", model=modelo, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# Contexto y pregunta en espa√±ol\n",
        "contexto = \"\"\"\n",
        "Lionel Messi es un futbolista argentino nacido en Rosario.\n",
        "Ha ganado siete Balones de Oro y es ampliamente considerado uno de los mejores jugadores de f√∫tbol de todos los tiempos.\n",
        "Jug√≥ muchos a√±os en el FC Barcelona antes de transferirse al Paris Saint-Germain en 2021.\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------\n",
        "pregunta = \"¬øD√≥nde juega Lionel Messi actualmente?\"\n",
        "\n",
        "# Realizar la predicci√≥n\n",
        "resultado = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar respuesta\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {resultado['answer']} (Confianza: {resultado['score']:.3f})\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "pregunta = \"¬øCu√°ntos balones de oro tiene?\"\n",
        "\n",
        "# Realizar la predicci√≥n\n",
        "resultado = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar respuesta\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {resultado['answer']} (Confianza: {resultado['score']:.3f})\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "pregunta = \"¬øJug√≥ en el Real Madrid?\"\n",
        "\n",
        "# Realizar la predicci√≥n\n",
        "resultado = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar respuesta\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {resultado['answer']} (Confianza: {resultado['score']:.3f})\")"
      ],
      "metadata": {
        "id": "prVaePY5Txdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>05. Resumir Textos</b></font>\n"
      ],
      "metadata": {
        "id": "JK13kIYNURdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, logging\n",
        "import torch\n",
        "\n",
        "# Modelo especializado para res√∫menes en espa√±ol\n",
        "modelo_nombre = \"mrm8488/bert2bert_shared-spanish-finetuned-summarization\"\n",
        "\n",
        "# Silenciar mensajes no cr√≠ticos\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Cargar modelo y tokenizador por separado\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_nombre)\n",
        "modelo = AutoModelForSeq2SeqLM.from_pretrained(modelo_nombre).to(device)\n",
        "\n",
        "# Crear pipeline de resumen (corregido aqu√≠)\n",
        "resumidor = pipeline(\"summarization\", model=modelo, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# Texto largo que deseas resumir\n",
        "texto_largo = \"\"\"\n",
        "OpenAI es una empresa dedicada a la investigaci√≥n en inteligencia artificial (IA) con sede en San Francisco, California.\n",
        "Fue fundada en 2015 por Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, John Schulman y Wojciech Zaremba,\n",
        "con el objetivo de desarrollar tecnolog√≠as avanzadas de IA de manera segura y beneficiosa para toda la humanidad.\n",
        "OpenAI ha desarrollado tecnolog√≠as reconocidas mundialmente, como GPT-3 y ChatGPT, herramientas ampliamente usadas en diversos campos.\n",
        "\"\"\"\n",
        "\n",
        "# Realizar resumen\n",
        "resumen = resumidor(texto_largo, max_length=50, min_length=20, do_sample=False)\n",
        "\n",
        "# Mostrar resultado\n",
        "print(f\"Resumen: {resumen[0]['summary_text']}\")\n"
      ],
      "metadata": {
        "id": "4nAmWqveUpd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>06. Traducci√≥n</b></font>"
      ],
      "metadata": {
        "id": "g9YY1A82XMRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opcinal, acelera calculos\n",
        "# !pip install sentencepiece"
      ],
      "metadata": {
        "id": "AtKm41DkUpq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator_es_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "texto_espanol = \"Estoy aprendiendo a traducir textos usando inteligencia artificial.\"\n",
        "traduccion_ingles = translator_es_en(texto_espanol)[0]['translation_text']\n",
        "\n",
        "print(\"Espa√±ol:\", texto_espanol)\n",
        "print(\"Ingl√©s:\", traduccion_ingles)"
      ],
      "metadata": {
        "id": "_czLME3rUpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>07. ChatBot</b></font>"
      ],
      "metadata": {
        "id": "L0xTBjPFXmX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias necesarias\n",
        "# Instalar las cosas y descargar el modelo puede llevar bastante tiempo\n",
        "# Pero la aplicaci√≥n queda muy chula con gradio\n",
        "\n",
        "!pip install -q transformers accelerate sentencepiece gradio\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gc\n",
        "\n",
        "class ChatbotEspanol:\n",
        "    def __init__(self):\n",
        "        # Verificar si hay GPU disponible en Colab\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "\n",
        "        # Cargar el modelo Gemma 2B adaptado para espa√±ol\n",
        "        # Este es uno de los mejores modelos livianos disponibles para espa√±ol que funcionar√° bien en Colab\n",
        "        self.model_name = \"LenguajeNaturalAI/leniachat-gemma-2b-v0\"\n",
        "\n",
        "        # Inicializar el tokenizer\n",
        "        print(\"Cargando tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Configuraci√≥n para optimizar el uso de memoria en Colab\n",
        "        print(\"Cargando modelo...\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Inicializar el historial de conversaci√≥n\n",
        "        self.history = []\n",
        "        print(\"¬°Chatbot listo!\")\n",
        "\n",
        "    def generate_response(self, user_input, history):\n",
        "        # Convertir el historial de Gradio al formato que necesitamos\n",
        "        self.history = []\n",
        "        for human, assistant in history:\n",
        "            self.history.append({\"role\": \"user\", \"content\": human})\n",
        "            if assistant:  # Puede ser None en la primera interacci√≥n\n",
        "                self.history.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "\n",
        "        # A√±adir el input actual del usuario\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Preparar la conversaci√≥n formateada para el modelo\n",
        "        conversation = []\n",
        "        for message in self.history:\n",
        "            conversation.append({\"role\": message[\"role\"], \"content\": message[\"content\"]})\n",
        "\n",
        "        # Generar tokens con la plantilla de chat adecuada\n",
        "        messages = self.tokenizer.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Tokenizar la conversaci√≥n\n",
        "        inputs = self.tokenizer(messages, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Generar respuesta\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Obtener la respuesta generada\n",
        "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Limpiar memoria CUDA para evitar fugas de memoria en Colab\n",
        "        if self.device == \"cuda\":\n",
        "            del inputs, outputs\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return response\n",
        "\n",
        "# Inicializar el chatbot\n",
        "chatbot = ChatbotEspanol()"
      ],
      "metadata": {
        "id": "G1FMuVKuYUmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear la interfaz con Gradio\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"green\")) as demo:\n",
        "    gr.Markdown(\"# ü§ñ Chatbot en Espa√±ol con Hugging Face\")\n",
        "    gr.Markdown(\"Este chatbot utiliza el modelo mlabonne/gemma-2b-it-spanish para generar respuestas en espa√±ol.\")\n",
        "\n",
        "    chatbot_interface = gr.ChatInterface(\n",
        "        chatbot.generate_response,\n",
        "        title=\"Conversaci√≥n\",\n",
        "        examples=[\n",
        "            \"¬øC√≥mo est√°s hoy?\",\n",
        "            \"Expl√≠came el calentamiento global como si tuviera 10 a√±os\",\n",
        "            \"¬øQu√© libros recomiendas para aprender sobre inteligencia artificial?\",\n",
        "            \"Escribe un poema corto sobre la primavera\",\n",
        "            \"Dame tres ideas para mejorar mi productividad\"\n",
        "        ]\n",
        "        # retry_btn=\"Reintentar\",\n",
        "        # undo_btn=\"Deshacer\",\n",
        "        # clear_btn=\"Limpiar\"\n",
        "    )\n",
        "\n",
        "# Lanzar la interfaz\n",
        "demo.launch(debug=False, share=True)"
      ],
      "metadata": {
        "id": "wCAK7EAgoS6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.close()"
      ],
      "metadata": {
        "id": "R-CmFALiYUqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>08. Embeddings</b></font>\n",
        "\n",
        "\n",
        "‚ÄãLos embeddings son representaciones matem√°ticas que transforman datos complejos y de alta dimensi√≥n, como palabras, im√°genes o sonidos, en vectores de n√∫meros reales en un espacio de menor dimensi√≥n. Esta transformaci√≥n facilita el procesamiento y an√°lisis de la informaci√≥n, preservando relaciones y patrones significativos entre los datos."
      ],
      "metadata": {
        "id": "r5_14eErr-sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Verificar si hay GPU disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando el dispositivo: {device}\")\n",
        "\n",
        "# Funci√≥n para aplicar Mean Pooling sobre las representaciones del modelo\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output.last_hidden_state  # √öltima capa oculta\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Frases en espa√±ol a convertir en embeddings\n",
        "sentences = [\"¬øC√≥mo est√° el clima hoy?\", \"¬øQu√© tiempo hace hoy?\", \"¬øCu√°l es la capital de Espa√±a?\"]\n",
        "\n",
        "# Cargar el tokenizador y el modelo en la GPU\n",
        "model_name = \"jinaai/jina-embeddings-v2-base-es\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
        "\n",
        "# Tokenizar las frases y moverlas a la GPU\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Obtener embeddings con el modelo (sin calcular gradientes)\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# Aplicar Mean Pooling para obtener una √∫nica representaci√≥n por frase\n",
        "embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
        "\n",
        "# Normalizar los embeddings (para c√°lculos de similitud)\n",
        "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "# Mostrar los embeddings generados\n",
        "print(\"Embeddings generados:\")\n",
        "print(embeddings)\n",
        "\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "TkbIQr52smOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "JBZAXINwsmRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix = torch.matmul(embeddings, embeddings.T)\n",
        "\n",
        "# Convertir a numpy para visualizar mejor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import ace_tools as tools\n",
        "\n",
        "similarity_matrix_np = similarity_matrix.cpu().numpy()\n",
        "\n",
        "# Crear un DataFrame para mostrar la matriz de similitud\n",
        "df_similarity = pd.DataFrame(similarity_matrix_np, index=sentences, columns=sentences)\n",
        "\n",
        "# Mostrar la matriz de similitud\n",
        "# tools.display_dataframe_to_user(name=\"Matriz de Similitud\", dataframe=df_similarity)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df_similarity, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Matriz de Similitud del Coseno\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hd2TkuoEsmUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>09. Algunos datasets interesantes</b></font>\n",
        "\n",
        "\n",
        "Aqu√≠ tienes un listado de algunos de los mejores datasets en espa√±ol disponibles gratuitamente para trabajar con modelos de lenguaje (LLMs):\n",
        "\n",
        "1. MLSUM - Corpus de res√∫menes de noticias en espa√±ol con m√°s de 250,000 art√≠culos de El Pa√≠s.\n",
        "1. Spanish Billion Words Corpus - Colecci√≥n masiva de textos en espa√±ol de diversas fuentes web.\n",
        "1. MC4-es - Subset en espa√±ol del Multilingual C4 de Google, con textos extra√≠dos de Common Crawl.\n",
        "1. OPUS - Colecci√≥n de textos paralelos traducidos en m√∫ltiples idiomas, incluyendo espa√±ol.\n",
        "1. esWiki-abstracts - Abstracts de la Wikipedia en espa√±ol.\n",
        "1. SQAC (Spanish Question Answering Corpus) - Dataset de preguntas y respuestas en espa√±ol.\n",
        "1. XNLI-es - Versi√≥n en espa√±ol del dataset de inferencia de lenguaje natural.\n",
        "1. PAWS-X-es - Corpus para evaluaci√≥n de par√°frasis en espa√±ol.\n",
        "1. HC3-Spanish - Dataset de conversaciones humano-chatbot en espa√±ol para detectar texto generado.\n",
        "1. CardioSentiBr - Opiniones de pacientes sobre hospitales en espa√±ol.\n",
        "1. TASS - Corpus de tweets en espa√±ol con an√°lisis de sentimiento.\n",
        "1. MeliSA - Dataset de opiniones de productos de Mercado Libre con an√°lisis de sentimiento.\n",
        "1. CAPITEL - Corpus anotado de la Real Academia Espa√±ola.\n",
        "1. Spanish SQuAD - Versi√≥n traducida del dataset SQuAD para respuesta a preguntas.\n",
        "1. MarIA datasets - Colecci√≥n de datasets para el entrenamiento de modelos en espa√±ol.\n",
        "\n",
        "1. Conjunto de Datos de Rese√±as de Amazon\n",
        "Este dataset recopila rese√±as de productos de Amazon en diversos idiomas, incluyendo el espa√±ol. Es ampliamente utilizado para tareas de clasificaci√≥n de texto y an√°lisis de sentimientos.\n",
        "\n",
        "1. Conjunto de Datos de Twitter para An√°lisis de Sentimientos\n",
        "Existen m√∫ltiples datasets que recopilan tuits en espa√±ol, etiquetados seg√∫n el sentimiento expresado. Estos conjuntos de datos son valiosos para entrenar modelos en tareas de an√°lisis de sentimientos y detecci√≥n de emociones.\n",
        "\n",
        "1. Corpus de Mensajes de Foros y Redes Sociales\n",
        "Algunas universidades y centros de investigaci√≥n han compilado conjuntos de datos que incluyen mensajes de foros y redes sociales en espa√±ol, √∫tiles para analizar interacciones y comportamientos en l√≠nea.\n",
        "\n",
        "1. Corpus de Noticias en Espa√±ol\n",
        "Recopilaciones de art√≠culos de noticias en espa√±ol, provenientes de diversas fuentes, que pueden ser utilizadas para tareas de resumen autom√°tico, clasificaci√≥n de noticias y detecci√≥n de temas.\n",
        "\n",
        "1. Conjunto de Datos de Comentarios en Redes Sociales\n",
        "Algunas plataformas acad√©micas ofrecen datasets que contienen comentarios y publicaciones de redes sociales en espa√±ol, √∫tiles para el an√°lisis de opiniones y estudios sociol√≥gicos.\n",
        "\n",
        "1. Corpus de Textos Legales en Espa√±ol\n",
        "Compilaciones de documentos legales y jur√≠dicos en espa√±ol, que pueden ser utilizadas para entrenar modelos especializados en el √°mbito legal.\n",
        "\n",
        "1. Corpus de Conversaciones en Espa√±ol\n",
        "Datasets que recopilan conversaciones y chats en espa√±ol, √∫tiles para entrenar modelos de di√°logo y chatbots.\n",
        "\n",
        "\n",
        "La mayor√≠a de estos datasets est√°n disponibles a trav√©s de Hugging Face Datasets, el Linguistic Data Consortium o los repositorios de sus creadores. Para casos espec√≠ficos como redes sociales, algunos investigadores han publicado datasets anonimizados que cumplen con los t√©rminos de servicio de las plataformas."
      ],
      "metadata": {
        "id": "z24kStUDvbbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adem√°s existe el paquete datasets de HuggingFace que incluye unas 300.000 bases, casi todas en ingl√©s, aunque no solamente\n",
        "\n",
        "[https://huggingface.co/datasets?p=99&sort=trending](https://huggingface.co/datasets?p=99&sort=trending)"
      ],
      "metadata": {
        "id": "WwUgAv6UyxOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "m7So0MnByLe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset de rese√±as de amazon (en ingl√©s)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Cargar el conjunto de datos de rese√±as de Amazon\n",
        "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n",
        "\n",
        "# Mostrar la primera rese√±a\n",
        "print(dataset[\"full\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61rp4cp9yLj8",
        "outputId": "ac395d7e-5123-47bd-a1d9-82c27bdb0b5c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rating': 5.0, 'title': 'Such a lovely scent but not overpowering.', 'text': \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\", 'images': [], 'asin': 'B00YQ6X8EO', 'parent_asin': 'B00YQ6X8EO', 'user_id': 'AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'timestamp': 1588687728923, 'helpful_vote': 0, 'verified_purchase': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset de amazon en espa√±ol\n",
        "dataset = load_dataset(\"SetFit/amazon_reviews_multi_es\")"
      ],
      "metadata": {
        "id": "D_4vKr-H0qcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras 5 rese√±as\n",
        "for i in range(5):\n",
        "    print(dataset['train'][i])"
      ],
      "metadata": {
        "id": "3g0HAFf-0O2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"darkorange\" size=\"6\"><b>10. C√≥mo entrenar un LLM para una tarea espec√≠fica (Fine Tuning)</b></font>"
      ],
      "metadata": {
        "id": "kHJmW6-G5Q9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch"
      ],
      "metadata": {
        "id": "luoPCqJsLGvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Cargar el conjunto de datos de cuentos en espa√±ol\n",
        "dataset = load_dataset(\"Fernandoefg/cuentos_es\")"
      ],
      "metadata": {
        "id": "ltBObaGPLO7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprime la estructura\n",
        "print(dataset)\n",
        "\n",
        "# ver cuento 0\n",
        "# print(dataset[\"train\"][0][\"content\"])\n",
        "\n",
        "# ver todos los titulos\n",
        "titulos = [cuento[\"title\"] for cuento in dataset[\"train\"]]\n",
        "print(titulos[:10])  # Muestra los primeros 10 t√≠tulos\n",
        "\n",
        "# buscar por un titulo\n",
        "# titulo_buscado = \"El Principito\"\n",
        "\n",
        "# cuento = next((cuento for cuento in dataset[\"train\"] if cuento[\"title\"] == titulo_buscado), None)\n",
        "\n",
        "# if cuento:\n",
        "#     print(f\"T√≠tulo: {cuento['title']}\\nAutor: {cuento['author']}\\n\\n{cuento['content']}\")\n",
        "# else:\n",
        "#     print(\"No se encontr√≥ el cuento.\")"
      ],
      "metadata": {
        "id": "SsoBbAV2LfhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se tokeniza el texto y se a√±aden las etiquetas (labels)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Cargar el tokenizador de un modelo GPT-2 en espa√±ol\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
        "\n",
        "# Tokenizar el texto de las historias y a√±adir labels\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(examples[\"content\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # Se necesita que labels sea igual a input_ids para la p√©rdida de entrenamiento\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Aplicar la tokenizaci√≥n al conjunto de datos\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "u0-C5Ap0L2WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Como el conjunto no tiene explicito ttrain y test hacemos un split\n",
        "dataset_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "# Crear un nuevo DatasetDict con train y test\n",
        "midataset = DatasetDict({\n",
        "    \"train\": dataset_split[\"train\"],\n",
        "    \"test\": dataset_split[\"test\"]\n",
        "})"
      ],
      "metadata": {
        "id": "sfN583MuPoOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Cargar el modelo GPT-2 en espa√±ol para generaci√≥n de texto\n",
        "model = AutoModelForCausalLM.from_pretrained(\"datificate/gpt2-small-spanish\")"
      ],
      "metadata": {
        "id": "6yqPUkeMND-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "output_dir=\"./resultados_historias\",  # Carpeta donde se guardan los resultados\n",
        "    run_name=\"entrenamiento_historias\",  # Nombre √∫nico para el experimento en WandB\n",
        "    report_to=\"none\",  # Desactiva WandB si no lo necesitas\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")"
      ],
      "metadata": {
        "id": "sClCHQzHNIKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=midataset[\"train\"],\n",
        "    eval_dataset=midataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "_wLsy0XkNK9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La tarea es tan demandante que no se puede llevar a cabo en colab\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3zI5tKegNNVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo ajustado\n",
        "from transformers import pipeline\n",
        "\n",
        "modelo_ajustado = pipeline(\"text-generation\", model=\"./resultados_historias\")\n",
        "\n",
        "# Generar una historia a partir de una frase inicial\n",
        "prompt = \"√ârase una vez en un bosque encantado,\"\n",
        "resultados = modelo_ajustado(prompt, max_length=200, num_return_sequences=1)\n",
        "\n",
        "print(resultados[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "OgIHjim7NP7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se puede ver las posibilidades son infinitas, aqu√≠ dejo una lista de tipos de entrenamiento para los que las librer√≠as de huggingFace est√°n dise√±adas:\n",
        "\n",
        "## Tareas de NLP para Modelos Preentrenados\n",
        "\n",
        "1. **Clasificaci√≥n de Texto**\n",
        "   - **Descripci√≥n:** Asignar categor√≠as predefinidas a fragmentos de texto.\n",
        "   - **Ejemplos:** Clasificaci√≥n de correos electr√≥nicos como \"spam\" o \"no spam\", an√°lisis de sentimientos en rese√±as de productos.\n",
        "\n",
        "2. **Reconocimiento de Entidades Nombradas (NER)**\n",
        "   - **Descripci√≥n:** Identificar y clasificar entidades como nombres de personas, organizaciones o lugares dentro de un texto.\n",
        "   - **Ejemplos:** En la frase \"Apple lanz√≥ el nuevo iPhone en California\", reconocer \"Apple\" como organizaci√≥n y \"California\" como lugar.\n",
        "\n",
        "3. **An√°lisis de Sentimientos**\n",
        "   - **Descripci√≥n:** Determinar la polaridad emocional de un texto, como positiva, negativa o neutral.\n",
        "   - **Ejemplos:** Analizar opiniones en redes sociales para evaluar la percepci√≥n p√∫blica de una marca.\n",
        "\n",
        "4. **Traducci√≥n Autom√°tica**\n",
        "   - **Descripci√≥n:** Convertir texto de un idioma a otro.\n",
        "   - **Ejemplos:** Traducir documentos del ingl√©s al espa√±ol.\n",
        "\n",
        "5. **Resumen de Texto**\n",
        "   - **Descripci√≥n:** Generar una versi√≥n condensada de un texto m√°s largo, manteniendo la informaci√≥n esencial.\n",
        "   - **Ejemplos:** Resumir art√≠culos de noticias o informes extensos.\n",
        "\n",
        "6. **Respuesta a Preguntas (QA)**\n",
        "   - **Descripci√≥n:** Proporcionar respuestas precisas a preguntas formuladas en lenguaje natural, bas√°ndose en un contexto dado.\n",
        "   - **Ejemplos:** Responder \"¬øQui√©n es el presidente de Francia?\" utilizando una base de datos de conocimiento.\n",
        "\n",
        "7. **An√°lisis de Sintaxis y Dependencias**\n",
        "   - **Descripci√≥n:** Descomponer oraciones para entender su estructura gramatical y las relaciones entre palabras.\n",
        "   - **Ejemplos:** Identificar el sujeto, verbo y objeto en una oraci√≥n.\n",
        "\n",
        "8. **Detecci√≥n de Lenguaje**\n",
        "   - **Descripci√≥n:** Identificar el idioma en el que est√° escrito un texto.\n",
        "   - **Ejemplos:** Determinar que un documento est√° en franc√©s.\n",
        "\n",
        "9. **Reconocimiento de Voz a Texto**\n",
        "   - **Descripci√≥n:** Transcribir contenido hablado en texto escrito.\n",
        "   - **Ejemplos:** Convertir grabaciones de entrevistas en texto para an√°lisis.\n",
        "\n",
        "10. **Conversi√≥n de Texto a Voz**\n",
        "    - **Descripci√≥n:** Generar audio hablado a partir de texto escrito.\n",
        "    - **Ejemplos:** Lectores de pantalla para personas con discapacidad visual.\n",
        "\n",
        "11. **Detecci√≥n de Parafraseo**\n",
        "    - **Descripci√≥n:** Identificar si dos frases tienen el mismo significado.\n",
        "    - **Ejemplos:** Detectar duplicados en respuestas de encuestas.\n",
        "\n",
        "12. **Resoluci√≥n de Coreferencias**\n",
        "    - **Descripci√≥n:** Determinar qu√© palabras en un texto se refieren a la misma entidad.\n",
        "    - **Ejemplos:** En \"Mar√≠a dijo que ella vendr√≠a\", identificar que \"ella\" se refiere a \"Mar√≠a\".\n",
        "\n",
        "13. **Extracci√≥n de Informaci√≥n**\n",
        "    - **Descripci√≥n:** Extraer datos estructurados espec√≠ficos de textos no estructurados.\n",
        "    - **Ejemplos:** Obtener fechas y lugares de eventos mencionados en art√≠culos de noticias.\n",
        "\n",
        "14. **Correcci√≥n Gramatical**\n",
        "    - **Descripci√≥n:** Identificar y corregir errores gramaticales en un texto.\n",
        "    - **Ejemplos:** Sugerir correcciones en redacciones estudiantiles.\n",
        "\n",
        "15. **Desambiguaci√≥n de Palabras**\n",
        "    - **Descripci√≥n:** Determinar el significado correcto de una palabra que tiene m√∫ltiples sentidos, seg√∫n el contexto.\n",
        "    - **Ejemplos:** En \"El banco est√° cerrado\", decidir si \"banco\" se refiere a una entidad financiera o a un asiento.\n",
        "\n",
        "16. **Generaci√≥n de C√≥digo**\n",
        "    - **Descripci√≥n:** Escribir c√≥digo de programaci√≥n basado en descripciones en lenguaje natural.\n",
        "    - **Ejemplos:** Generar una funci√≥n en Python que calcule la suma de una lista de n√∫meros.\n",
        "\n",
        "17. **Conversi√≥n de C√≥digo a Lenguaje Natural**\n",
        "    - **Descripci√≥n:** Explicar en lenguaje natural lo que hace un fragmento de c√≥digo.\n",
        "    - **Ejemplos:** Describir la funcionalidad de una funci√≥n en JavaScript.\n",
        "\n",
        "18. **Detecci√≥n de Humor o Sarcasmo**\n",
        "    - **Descripci√≥n:** Identificar tonos humor√≠sticos o sarc√°sticos en textos.\n",
        "    - **Ejemplos:** Analizar tweets para detectar sarcasmo.\n",
        "\n",
        "19. **An√°lisis de Temas**\n",
        "    - **Descripci√≥n:** Identificar temas o t√≥picos predominantes en un conjunto de documentos.\n",
        "    - **Ejemplos:** Descubrir que un conjunto de art√≠culos de noticias trata principalmente sobre econom√≠a y salud.\n",
        "\n",
        "20. **Generaci√≥n de Preguntas**\n",
        "    - **Descripci√≥n:** Crear preguntas relevantes basadas en un texto dado.\n",
        "    - **Ejemplos:** Generar preguntas de comprensi√≥n lectora para un p√°rrafo educativo.\n",
        "\n",
        "21. **Conversi√≥n de Formato de Texto**\n",
        "    - **Descripci√≥n:** Transformar texto entre diferentes formatos o estilos.\n",
        "    - **Ejemplos:** Convertir texto plano en formato JSON o XML.\n",
        "\n",
        "22. **Detecci√≥n de Plagio**\n",
        "    - **Descripci√≥n:** Identificar contenido copiado o similar a otros textos existentes.\n",
        "    - **Ejemplos:** Verificar la originalidad de trabajos acad√©micos.\n",
        "\n",
        "23. **An√°lisis de Redes Sociales**\n",
        "    - **Descripci√≥n:** Analizar contenido de plataformas sociales para obtener insights.\n",
        "    - **Ejemplos:** Evaluar la reacci√≥n del p√∫blico ante un evento reciente.\n",
        "\n",
        "24. **Clasificaci√≥n de Intenciones**\n",
        "    - **Descripci√≥n:** Identificar la intenci√≥n detr√°s de una consulta o mensaje.\n",
        "    - **Ejemplos:** Determinar si una pregunta en un chatbot es una consulta de informaci√≥n o una solicitud de acci√≥n.\n",
        "\n",
        "25. **Detecci√≥n de Entidades de Producto**\n",
        "    - **Descripci√≥n:** Identificar menciones de productos espec√≠ficos en textos.\n",
        "    - **Ejemplos:** Encontrar referencias a modelos de tel√©fonos en rese√±as.\n",
        "\n",
        "26. **An√°lisis de Competencia**\n",
        "    - **Descripci√≥n:** Evaluar menciones y sentimientos hacia competidores en el mercado.\n",
        "    - **Ejemplos:** Analizar c√≥mo se perciben diferentes marcas en comentarios de clientes.\n",
        "\n",
        "27. **Detecci√≥n de Noticias Falsas**\n",
        "    - **Descripci√≥n:** Identificar la veracidad de la informaci√≥n presentada en un texto.\n",
        "    - **Ejemplos:** Evaluar si un art√≠culo de noticias contiene informaci√≥n falsa o enga√±osa.\n",
        "\n",
        "28. **Detecci√≥n de Contenido Ofensivo**\n",
        "    - **Descripci√≥n:** Identificar lenguaje inapropiado o da√±ino en textos.\n",
        "    - **Ejemplos:** Filtrar comentarios ofensivos en plataformas en l√≠nea.\n",
        "\n",
        "29. **An√°lisis de Discurso**\n",
        "    - **Descripci√≥n:** Examinar textos para entender estructuras argumentativas o ret√≥ricas.\n",
        "    - **Ejemplos:** Analizar discursos pol√≠ticos para identificar argumentos clave.\n",
        "\n",
        "30. **Generaci√≥n de Descripciones de Im√°genes**\n",
        "    - **Descripci√≥n:** Crear descripciones textuales basadas en el contenido de im√°genes.\n",
        "    - **Ejemplos:** Describir el contenido de una fotograf√≠a\n"
      ],
      "metadata": {
        "id": "HnIQhccBT6CV"
      }
    }
  ]
}